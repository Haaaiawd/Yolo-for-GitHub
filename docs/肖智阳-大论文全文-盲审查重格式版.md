应用于海滩监控系统的小目标检测和行为识别算法及其系统研究 

<a name="_page1_x87.00_y132.92"></a>中文摘要 

随着近年来各地滨海旅游业的迅猛发展，海滩游客呈现出人群密度高，各区 域游客分布分散且复杂，依靠传统人工监控画面对游客风险事件的预防能力愈显 不足。伴随人工智能技术的高速发展，利用深度学习方法开发出适配海滩游客特 征的小目标检测及其后续风险行为识别系统，划分重点区域并对其进行实时监 测，能有效减少人员进入危险区域的次数并对已进入的游客进行劝返，对提升海 滩安全建设和保障游客安全有重要意义。本课题主要研究工作及创新内容如下：

- 1）针对复杂海滩场景模式下小目标游客检测精度不足的问题，本课题提

出一种多层特征图信息融合的海滩小目标检测方法，通过强化特征图信息与结合 上下文信息来提高小目标游客的检出率。通过采集监控视频，间隔 30  s抽取监 控中 1帧图像形成海滩小目标游客数据集，最终得到共计 1227张海滩场景小目 标游客图像集。设计全局跨阶段注意力机制并结合多尺度特征连接以及损失函数 等多个角度在实时性满足要求的前提下与原始 YOLOv5 网络相比查准率提升 2.00%，查全率提升 5.33%，平均精度均值提升 4.36%，并且获得比其他主流方 法更好的主观视觉评价，有效保障危险区域中游客的人身安全。 

- 2）针对溺水行为中能被检测到的骨架点信息缺失导致行为判断准确率较

低的问题，本课题提出一种融合改进 Alphapose的时空图卷积网络的溺水异常行 为识别算法。在视频网站收集溺水相关视频，裁剪相关片段，形成溺水异常行为 动作视频集，最终得到共计 378段水面溺水行为动作视频集。充分利用人体骨架 点，增强对游客信息姿态估计能力，结合时间与空间维度利用有限游客关节点信 息构建图神经网络，最后判断出游客是否处于溺水异常行为。本课题所提方法在 骨架点提取上取得比 HRNet网络更加精确的结果，相比原始 ST-GCN网络溺水 行为识别准确率提升 35.66%，在海滩场景下能有效区分溺水行为与非溺水行为。

- 3）设计并完成海滩监控的小目标检测和行为识别系统研究，基于多层特

征图信息融合的海滩小目标检测方法与融合改进的Alphapose的时空图卷积行为 识别方法，设计相关软、硬件环境与系统整体流程，利用 Qt  Designer搭建相关 GUI界面，通过实物部署完成系统相关功能测试，验证系统的可行性，通过系统 实物运行评估和分析可以看出，本系统各部分功能运行正常，衔接连贯，能够有 效进行相应的小目标检测与后续行为识别，保障了游客的人身安全。 

关键词：智能监控；小目标检测；姿态估计；行为识别  

Small Object Detection and Behavior Recognition Algorithm and System for Beach Monitoring <a name="_page2_x87.00_y144.92"></a>System 

Abstract

In  recent  years,  coastal  tourism  has  grown  rapidly,  leading  to  a  high concentration of beachgoers and a complex distribution of tourists across different beach  regions.  The  ability  to  prevent  risk  events  for  tourists  through  traditional manual  monitoring  is  becoming  more  and  more  inadequate.  With  the  rapid development of artificial intelligence technology, the deep learning method is used to develop a small object detection and subsequent risk behavior recognition system adapted to the characteristics of beach tourists, and the key areas are divided and monitored in real time, which can effectively limit the number of individuals entering hazardous zones and successfully convince visitors to turn back. It is crucial to bolster the safety infrastructure at beaches to ensure the protection of tourists. The main research work and innovative content are as follows:  

1) In an effort to the problem of insufficient accuracy in detecting small object 

in complex beach scenes, this paper proposes a beach small object detection method that integrates multi-layer feature map information to improve the detection rate of small object tourists from the perspective of contextual information and reinforced feature map information fusion. By collecting surveillance videos and extracting one frame from the surveillance video every 30 seconds to form a beach small object tourist dataset, resulting in a total of 1227 beach scene small object tourist image sets were obtained. A global cross-stage attention mechanism is designed and combined with multi-scale feature connections and loss functions from multiple perspectives to improve the precision by 2.00%, recall by 5.33%, and mean average precision by 4.36%  compared  to  the  original  YOLOv5  network  while  meeting  real-time requirements. It achieves better subjective visual evaluation than other mainstream methods, effectively ensuring the personal safety of tourists in dangerous areas.  

2) In response to the problem of low accuracy of behavior judgment due to the 

lack of information of skeleton points that can be detected in drowning behavior; this paper  proposes  an  abnormal  drowning  behavior  recognition  algorithm  based  on improved Alphapose spatio-temporal graph convolutional network. Drowning-related videos are collected from public video websites, and relevant clips are cropped to form a video set of abnormal behaviors and actions of drowning, resulting in a total of 378  water  surface  drowning  behavior  action  video  sets.  By  fully  utilizing  human skeleton  points  to  enhance  the  ability  to  estimate  tourist  information  posture  and combining  time  and  space  dimensions  to  construct  a  graph  neural  network  using limited tourist joint point information, it is finally determined whether tourists are in drowning or other abnormal behaviors. The method proposed in this project achieves more  accurate  results  than  the  HRNet  network  in  skeleton  point  extraction  and improves the accuracy of drowning behavior recognition by 35.66% compared to the original  ST-GCN  network.  It  can  effectively  distinguish  between  drowning  and non-drowning behaviors in beach scenes.  

3) Designing and completing research on small object detection systems for 

beach monitoring along with behavioral recognition based on multi-layer feature map information  fusion  methods  for  detecting  small  object  on beaches  combined  with improved  Alphapose  integrated  spatiotemporal  graph  convolutional  behavioral recognition methods while setting up relevant software/hardware environments along with  overall  system  processes  using  Qt  Designer  building  related  GUI  interfaces before  deploying  physical  objects  completing  system-related  functional  testing verifying system feasibility through physical operation evaluation analysis showing that all parts function normally coherently effectively performing corresponding small object  detection  followed  by  subsequent  behavioral  recognition  ensuring  personal safety for tourists. 

Key words: Intelligent monitoring; Small object detection; Pose estimation; Behavior recognition

目录

[中文摘要 ...................................................................................................................... I ](#_page1_x87.00_y132.92)[Abstract................................................................................................................... II ](#_page2_x87.00_y144.92)[第一章 引言 ............................................................................................................... 1 ](#_page7_x87.00_y76.92)

1. [研究背景与意义 .......................................................................................... 1](#_page7_x87.00_y126.92)
1. [国内外研究现状 .......................................................................................... 3](#_page9_x87.00_y76.92)
1. [小目标检测研究现状 ....................................................................... 3](#_page9_x87.00_y120.92)
1. [行为识别研究现状 ........................................................................... 7](#_page13_x87.00_y590.92)
3. [论文研究主要内容与目的 .........................................................................11](#_page17_x87.00_y76.92)
4. [论文组织结构安排 .................................................................................... 12](#_page18_x87.00_y76.92)

[第二章 相关技术与理论基础概述 ......................................................................... 13](#_page19_x87.00_y78.92)

1. [卷积神经网络 ............................................................................................ 13](#_page19_x87.00_y128.92)
1. [卷积层 ............................................................................................. 13](#_page19_x87.00_y487.92)
1. [池化层 ............................................................................................. 14](#_page20_x87.00_y538.92)
1. [全连接层 ......................................................................................... 15](#_page21_x87.00_y482.92)
4. [激活函数 ......................................................................................... 16](#_page22_x87.00_y270.92)
2. [YOLO算法基本原理.................................................................................. 16](#_page22_x87.00_y628.92)
1. [YOLO算法思想............................................................................... 16](#_page22_x87.00_y684.92)
1. [YOLO算法模型结构....................................................................... 17](#_page23_x87.00_y611.92)
3. [图卷积网络基本原理 ................................................................................ 18](#_page24_x87.00_y535.92)
1. [图卷积网络思想 ............................................................................. 18](#_page24_x87.00_y591.92)
1. [图卷积网络模型结构 ..................................................................... 20](#_page26_x87.00_y78.92)
4. [本章小结 .................................................................................................... 20 ](#_page26_x87.00_y528.92)[第三章 多层特征图信息融合的海滩小目标检测算法研究 ................................. 21](#_page27_x87.00_y78.92)
1. [YOLOv5网络结构分析.............................................................................. 21](#_page27_x87.00_y148.92)
1. [多层特征图信息融合的海滩小目标检测算法模型 ................................ 21](#_page27_x87.00_y532.92)
1. [主干网络端优化 ............................................................................. 22](#_page28_x87.00_y398.92)
1. [颈部融合端优化 ............................................................................. 23](#_page29_x87.00_y674.92)
1. [损失函数优化 ................................................................................. 25](#_page31_x87.00_y78.92)
3. [实验及结果分析 ........................................................................................ 26](#_page32_x87.00_y468.92)
1. [数据集的建立与实验环境配置 ..................................................... 26](#_page32_x87.00_y536.92)
1. [实验策略与评价指标 ..................................................................... 27](#_page33_x87.00_y358.92)
3. [消融实验 ......................................................................................... 28](#_page34_x87.00_y338.92)
3. [算法性能测试及对比实验 ............................................................. 29](#_page35_x87.00_y638.92)
3. [Visdrone数据集对比实验 ............................................................... 32](#_page38_x87.00_y258.92)
4. [本章小结 .................................................................................................... 32 ](#_page38_x87.00_y634.92)[第四章 基于时空图卷积网络的溺水异常行为识别算法研究 ............................. 34](#_page40_x87.00_y78.92)
1. [骨架关键点数据处理 ................................................................................ 34](#_page40_x87.00_y148.92)
1. [骨架关键点标定格式 ..................................................................... 34](#_page40_x87.00_y192.92)
1. [基于 Alphapose的骨架关键点提取方法 ...................................... 34](#_page40_x87.00_y517.92)
3. [Alphapose算法优化......................................................................... 36](#_page42_x87.00_y524.92)
2. [融合改进 Alphapose的时空图卷积网络 ................................................. 37](#_page43_x87.00_y118.92)
1. [基于时空图卷积网络的人体行为识别方法 ................................. 37](#_page43_x87.00_y186.92)
1. [融合改进 Alphapose的时空图卷积网络模型结构 ...................... 39](#_page45_x87.00_y371.92)
3. [实验及结果分析 ........................................................................................ 40](#_page46_x87.00_y314.92)
1. [数据集的建立 ................................................................................. 40](#_page46_x87.00_y370.92)
1. [实验环境配置与评价指标 ............................................................. 41](#_page47_x87.00_y78.92)
1. [姿态估计方法对比实验 ................................................................. 41](#_page47_x87.00_y436.92)
1. [算法性能测试及对比实验 ............................................................. 42](#_page48_x87.00_y598.92)
4. [本章小结 .................................................................................................... 43 ](#_page49_x87.00_y625.92)[第五章 海滩监控的小目标检测和行为识别系统研究 ......................................... 45](#_page51_x87.00_y78.92)
1. [系统需求与实现环境 ................................................................................ 45](#_page51_x87.00_y128.92)
1. [应用背景与功能需求 ..................................................................... 45](#_page51_x87.00_y172.92)
1. [开发环境与开发工具 ..................................................................... 46](#_page52_x87.00_y118.92)
2. [系统总体设计 ............................................................................................ 46](#_page52_x87.00_y548.92)
1. [系统硬件框架 ................................................................................. 46](#_page52_x87.00_y616.92)
1. [系统软件框架 ................................................................................. 47](#_page53_x87.00_y411.92)
1. [系统整体流程 ................................................................................. 48](#_page54_x87.00_y78.92)
3. [系统功能模块与可视化界面搭建 ............................................................ 48](#_page54_x87.00_y469.92)
1. [系统主界面 ..................................................................................... 48](#_page54_x87.00_y525.92)
1. [接入监控视频界面 ......................................................................... 49](#_page55_x87.00_y556.92)
1. [海滩小目标检测界面 ..................................................................... 50](#_page56_x87.00_y605.92)
1. [溺水行为异常识别界面 ................................................................. 51](#_page57_x87.00_y655.92)
4. [系统功能测试与实现效果 ........................................................................ 52](#_page58_x87.00_y637.92)
1. [选择视频流与小目标检测功能测试 ............................................. 53](#_page59_x87.00_y78.92)
1. [溺水行为识别功能测试 ................................................................. 54](#_page60_x87.00_y224.92)
3. [系统实际部署运行状态 ................................................................. 55](#_page61_x87.00_y78.92)
5. [本章小结 .................................................................................................... 56 ](#_page62_x87.00_y458.92)[总结与展望 ............................................................................................................... 57 ](#_page63_x87.00_y76.92)[总结 ................................................................................................................... 57 ](#_page63_x87.00_y126.92)[展望 ................................................................................................................... 59 ](#_page65_x87.00_y76.92)[参考文献 ................................................................................................................... 60 ](#_page66_x87.00_y72.92)[在学期间的研究成果及发表的学术论文 ............................................................... 65](#_page71_x87.00_y72.92)

v 
福州大学工程硕士学位论文 ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.003.png)

<a name="_page7_x87.00_y76.92"></a>第一章 引言 

1. 研究背景与意义<a name="_page7_x87.00_y126.92"></a> 

随着我国经济实力稳中向好的发展，居民收入也日益增长，人们对精神生活 的追求愈发强烈。我国在 2021年发布的海洋十四五规划进一步阐明了其中的海 洋经济和生态战略，每年暑期滨海旅游业广受各个年龄段游客的喜爱，随着赴海 游热度不断提升，游客的人身安全问题也逐渐显露出来，每年都会存在游客因为 各式各样的原因出现溺水等安全事件[\[1\]](#_page66_x87.00_y122.92)，一些常见的海滩安全事故示例图如图 1-1所示。例如在福建省晋江市金沙湾景区 2021年 6月一名 15岁的云南小伙在 游泳时被海水卷走，所幸被海上义务救援人员成功救回；2018年 10月救援人员 在该海域成功营救五名因不熟悉潮汐而溺水的游客。一方面是游客自身安全意识 薄弱，另一方面也是由于目前我国海滩安全建设仍然处于起步阶段，对已有的海 洋灾害研究调查集中在极端灾害事件中，忽略日常发生的海滩游客安全事故，导 致事故发生到进行救援之间存在较大延迟。同时现阶段的海滩安全系统，主要依 靠人工不间断地把守在监控摄像头画面前，通过人眼不断扫视画面来预防与响应 安全事故的发生。由于安全事故的低概率性以及画面较大且复杂的特点，人工很 容易出现视觉疲劳，使得监控往往成为事故发生之后的复查手段，起不到预防事 故发生的作用。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.004.jpeg)

- a）海洋自然灾害           （b）遇难的溺水者           （c）无人看管的儿童          

图 1-1 海滩常见安全事故示例图 

近年来，人工智能技术取得飞速发展，智能化产品及应用在日常生活中逐步 普及，城市生活也逐渐过渡向智能化发展。智慧城市利用新一代信息技术巧妙地 将城市功能系统与人工智能技术结合，使得城市生活具有自感知、自适应、自调 节等各式各样丰富多彩的新变化。智能监控作为智慧城市至关重要的一环，被广 泛运用在城市安防建设、企业生产管理等场所。但仍有一些场所依旧需要以人工 监视为主体进行实时监控，例如海滩景区、大流量车流及人流检测系统等。相较 于城市街道和企业生产车间中架设的监控设备，海滩监控由于海风侵蚀、沙滩松

软、潮汐规律等环境限制，无法架设在距离海滩较近的位置。在监控画面中获取 到单个游客目标占整个画面的比重很小，而现有的智能监控方案都是服务于室内 环境或者监控区域附近，出现在画面中的目标往往占比较高且目标特征细节丰富 易于帮助系统做出准确判断。此类智能监控方案无法满足复杂场景下小目标的检 测及其风险行为识别的需求，因此，借助深度学习的方法开发出适配复杂场景下 的小目标检测及其后续风险行为识别系统，有效降低人员在危险区域出没的频 率，同时对劝返途中的游客行为实时进行监控，一旦意外发生便可及时进行援护， 对加强海滩安全系统建设与保障游客人身安全具有重要意义。 

根据《2021年中国海洋经济统计公报》[\[2\]](#_page66_x87.00_y159.92)显示，滨海旅游尚未恢复到疫情前 的水平，是因为疫情的多点散发，但是随着政府出台的助企纾困和刺激消费政策， 滨海旅游市场逐渐回暖。2021全年实现增加值 15297亿元，比上年增长 12.8%， 占主要海洋产业增值构成的 44.9%。由此可见滨海旅游业对我国海洋产业发展的 重要性，而游客作为滨海旅游业的主力军，其游玩环境、游玩体验、人生安全对 滨海旅游业的发展又至关重要。本课题致力于智能海滩监控系统研究，从保障游 客人生安全角度出发，有效提升游玩环境的安全系数，增加游客游玩时的幸福感。 同时将人工智能技术引入传统滨海旅游业中，切实围绕国家十四五期间提出的发 挥科技创新在海洋经济高质量发展中的引领作用。本课题的研究拓展了科技创新 在海洋经济中的运用，推动传统技术产业升级和优化改造，弥补了科技创新长久 以往在我国海洋经济中贡献不足的问题，致力于帮助我国迈向“海洋强国”行列， 助力海洋产业经济快速增长。 

同时本课题所研究的小目标检测与行为识别技术也将助力社会其他产业的 发展，华为提出 5G时代十大应用场景白皮书[\[3\]](#_page66_x87.00_y179.92)，许许多多产业场景都离不开小 目标的检测与行为识别，例如：自动驾驶、联网无人机、智能监控、国防安全等， 本课题的研究可应用产业举例图如图 1-2所示。这些产业在实际应用场景中往往 都存在需要特定关注的小目标并执行相应的后续行为监督，因此优秀的小目标检 测及行为识别方法将助力这些产业更加快速、更加智能地发展。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.005.jpeg)

- a）自动驾驶        （b）联网无人机        （c）智能监控         （d）国防安全         

图 1-2 本课题研究可应用产业举例图 

2. 国内外研究现状<a name="_page9_x87.00_y76.92"></a> 
1. 小目标检测研究现状<a name="_page9_x87.00_y120.92"></a> 

更复杂、更高层次的机器视觉任务，如自动驾驶、图像拼接、目标追踪、场 景理解、实例分割等任务均需以目标检测所提供的数据为基础，一个好的检测效 果对后续任务开展起到至关重要的作用。小目标自身存在着语义特征缺乏、图像 占比低等不足，因此小目标检测长期以来都是目标检测领域的核心问题之一。在 传统方法中，通常采取四个主要步骤来解决小目标检测问题：首先，对待检测区 域进行选择，使用区域建议或者滑动窗口的方法在图像中确定候选区域，区域中 可能包含待检测的目标位置与大小；接着，使用人工设计的特征对各个候选区域 进行特征向量提取；然后，采用机器学习中的监督学习算法，如最邻近节点算法

- K-Nearest Neighbor，KNN）、自适应增强算法（Adaptive Boost，AdaBoost）或 支持向量机（Support Vector Machine，SVM）等，对每个候选区域中选取到的特 征向量进行分类，判断该特征最终属于哪一个类别目标；最后，进行一些后处理 操作用于去除重叠或者冗余的检测结果并展示最终的结果。 

2001年 Paul与 Michae[l\[4\]](#_page66_x87.00_y199.92)提出一种基于 AdaBoost和级联分类器的方法解决 实时性要求较高的小目标视觉对象检测，该方法通过“积分图像”的方式达到在 常数时间、任意位置计算特征，采用级联的方式将复杂的分类器组合快速排除背 景信息，但该方法没有考虑到目标在图像中可能产生的形变与目标分布的空间布 局，最终导致检测精度的下降。2009年 Li等人[\[5\]](#_page66_x87.00_y253.92)采用基于异常检测算法的高光 谱目标检测算法，巧妙地运用光谱分析技术将小目标与背景信息进行区分，采用 光谱角映射对数据进行降维，达到无需先验信息的目的，通过马氏距离与自适应 阈值的比较确定小目标位置信息，但该方法没有找到很好的预处理方式在保留目 标光谱特征信息的同时降低背景的复杂性。2011年 Xing等人[\[6\]](#_page66_x87.00_y290.92)引入一种用窄带 雷达检测和参数估计高速小目标的算法，通过横向频率调制率、模糊多普勒频率 和折叠因子三个方面估计目标运动参数，针对高速运动的小目标取得很好的检测 效果，但该方法仅适用于高速小目标，缺乏一定的普适性。2020 年谷雨等人[\[7\] ](#_page66_x87.00_y344.92)针对红外图像弱小目标检测设计出一种基于改进多尺度分形特征（Improved Multi-Scale Fractal Feature，IMFFK）的小目标检测算法，结合多尺度分形特征增 强图像质量，并在分形维数计算公式中应用地毯覆盖法，采用自适应阈值分割得 到感兴趣的小目标，该方法能适用于小目标与大目标同时存在的场景，但在某些 复杂场景下仍存在较多虚警，且对处理图像的尺度比较单一，仍需进一步提升， 基于改进多尺度分形特征的红外图像弱小目标检测流程图如图 1-3所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.006.jpeg)

- a）输入图像       （b）多尺度分析特征图            （c）增强图像            （d）检测结果  

图 1-3 基于改进多尺度分形特征的红外图像目标检测算法流程框图 

2012年大规模视觉识别挑战赛上 AlexNet[\[8\]](#_page66_x87.00_y381.92)以大比分优势远超第二名碾压取 得竞赛冠军，在目标检测领域人们渐渐意识到以卷积神经网络（Convolutional Neural Network，CNN）为代表的深度学习方法超乎想象的作用及优势，传统的 目标检测方法逐渐被深度学习方法所吸收结合并改进，同时基于深度学习的方法 很快成为了研究热点。例如，2021年鞠默然等人[\[9\]](#_page66_x87.00_y435.92)提出一种简单高效的实时红外 弱小目标检测网络，利用自适应感受野与空间注意力机制使得不同区域之间的上 下文信息与相关性得到加强，但该方法未对不同场景和光照条件下的红外弱小目 标进行相应的实验检测以及对网络鲁棒性的验证。2022年 Huang与 Liu[\[10\]](#_page66_x87.00_y472.92)提出 一种密集检测网络 Libra EBox，采用 Libra椭圆采样和残差低层特征增强来解决 小目标检测中的尺度不平衡问题，实验结果表明该方法超越同时期的大多数密集 检测网络，但并未与其他基于椭圆框检测的网络进行对比，缺乏一定的普适性。 同样还有朱威等人[\[11\]](#_page66_x87.00_y509.92)在 2022 年提出的引入注意力机制的轻量级小目标检测网 络，通过注意力机制增强网络感知能力，利用残差结构与深度可分离卷积减少网 络计算量，提高运算效率，但该方法并未对其运行时间、资源消耗、能耗等方面 进行详细分析与评估，无法判断其实际工程应用的可行性与效率。现阶段学者们 经过研究发现可以通过以下几类方法来改进基于深度学习的小目标检测： 

- 1）数据增强，小目标往往在整个数据集中所占比重较低，且分布不均匀，

使得感知器难以提取到具有鉴别力的特征，通过对输入图片进行一系列的旋转、 伸缩、调色、复制等操作，在保证增强后的数据与原始数据在特性及语义方面保 持一致情况下，增加小目标的数量与多样性，增强小目标背景与场景的变化性， 从而提高整个网络模型对小目标检测的泛化能力。例如，Cubuk等人[\[12\]](#_page66_x87.00_y546.92)提出一种 自动搜索的数据增强策略（AutoAugment），通过设计搜索空间中的子策略的应 用概率与幅度，使得神经网络在目标数据集上获得更高的准确率，但 AutoAugment的搜索过程需要大量的计算资源和时间，不适合一些小型或低资源 的任务。Wang等人[\[13\]](#_page66_x87.00_y600.92)设计出一种基于尺度感知的数据自动增强方法来应对建筑 工地的大场景与小目标检测问题，通过学习数据增强策略，提高数据集的多样性 与难度从而提升网络检测效果，但在更复杂和多变的建筑工地环境下，如何进一 步提高数据自动增强方法的效率和稳定性是该方法需要进一步解决的问题。Yin 等人[\[14\]](#_page66_x87.00_y637.92)构建出一种针对目标检测的数据增强方式，利用随机亮度、随机对比度、 随机旋转、随机裁剪等图像分类改进手段，将目标从边界框中提取出来，经过处 理后，使用类似 MixUp的融合方式达到增强数据的效果，并引入 XIOU保证目 标合成过程中的交集比例，但该方法在小目标和目标背景缺乏的场景下效果不 佳，数据样本小且不平衡。Kisantal 等人[\[15\]](#_page66_x87.00_y674.92)提出一种“Copy-Pasting  strategies” 的数据增强策略来增加小目标在图像中出现的频率，使得网络更好地捕捉小目标 信息从而提高小目标的检出率，但过度的数据增强可能导致网络训练出现过拟合 现象，同时此方法可能会引入一些不真实或不自然的场景，需要考虑场景一致性 和背景融合，图 1-4为“Copy-Pasting strategies”数据增强策略示意图。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.007.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.008.png)

图 1-4 “Copy-Pasting strategies”方法示意图 

- 2）多尺度融合，现有的绝大多数基于卷积的目标检测方法主要通过卷积

操作获得的特征图来进行选择预测，浅层特征图由于感受野较小，空间分辨率较 高可以很好的寻找到小目标的位置，但其特征的语义信息表达能力欠佳，召回率 低。而深层网络特征图具有语义信息丰富，感受野较全面的特点，但由于经过多 次卷积操作，小目标在特征图中所占像素值越来越少，甚至直接丢失小目标信息， 导致无法定位。因此结合多尺度使得网络可以在较高召回率下对小目标进行准确 的分类和定位。例如，Shrivastava与 Gupta[\[16\]](#_page66_x87.00_y728.92)通过在 Faster-RCNN自身的多尺度 结构中增加一个语义分割网络，用于提供上下文信息和反馈信号，帮助网络指导 注意力聚焦区域与学习检测器，提高目标检测精度，但方法过多地增加了网络的 计算复杂度降低了时效性。Cai等人[\[17\]](#_page67_x87.00_y72.92)则通过一个统一的多尺度深度卷积神经网 络（MS-CNN）完成快速的目标检测，MS-CNN在候选区域子网络中使用感受野 与不同尺度目标进行匹配，利用统一的网络优化进行端到端的学习，在包含大量 小目标的数据集中取得很好的检测效果，未来如何进一步提升目标检测的准确率 和速度以及如何适应更复杂和多样化的场景和任务是待解决的难点。Bell等人[\[18\] ](#_page67_x87.00_y126.92)设计出一种利用内外信息的目标检测网络（Inside-Outside Net，ION），使用跳跃 池化在多尺度和层次上提取特征信息，增强了目标检测的细节和抽象能力，取得 佳绩，但网络结构复杂，需要运用大量的计算资源与训练时间，同时在小目标的

检测效果上仍存在一定的困难。Bharat与 Larry[\[19\]](#_page67_x87.00_y180.92)提出一种称为图像金字塔的尺 度归一化，将不同尺度大小的对象实例梯度作为图像的尺度函数进行反向传播， 证明了在目标检测领域中图像金字塔和尺度的重要性不言而喻。图 1-5展示了包 含多尺度方法的网络结构。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.009.jpeg)

图 1-5 多尺度方法示意图 

- 3）锚框设计，锚框最初是为解决滑窗遍历速度过慢的弊端所提出的，由

于大目标与小目标之间的形态差异过大，导致提前预设的锚框无法顾及大中小目 标，得到一个合适的尺度，因此设计出满足不同尺寸目标匹配的合适锚框，可以 很大程度提高检测精度。例如，Zhong等人[\[20\]](#_page67_x87.00_y234.92)提出一种通用方法来优化目标检测 中的锚框形状，使得网络可以自适应数据分布，几乎不用增加额外的训练成本也 不影响后续的推理时间与内存消耗，但目前该方法适应场景不够丰富，后续可以 扩展到更加复杂的检测任务中，如多尺度、多类别、多任务等。Zhang 等人[\[21\] ](#_page67_x87.00_y288.92)设计一种轻量化的网络结构通过锚点离散化与感受野信息来处理不同尺度的人 脸识别问题，采用的新锚点密集化策略使得不同类型的锚点图像上具有相同的密 度从而提升网络的召回率，但在一些具有复杂背景或遮挡的场景中，该方法仍有 提升空间。Zhu等人[\[22\]](#_page67_x87.00_y342.92)创立一种新的锚点设计原则，该原则应对微小的人脸场景 下，具有优越的尺度不变性能，通过新的期望最大重叠分数从理论上解释了重叠 度的问题，但方法的精度提升是依靠牺牲速度与内存消耗所实现的，难于实际应 用落地。Du等人[\[23\]](#_page67_x87.00_y396.92)为解决红外弱小目标检测任务，根据 ResNet50的浅层设计出 有效的小锚框，以保证小目标实例能够正确地被网络使用，该方法显示出无限优 势和巨大潜力，图 1-6展示了锚框设计理念。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.010.png)

图 1-6 锚框设计理念示意图 

- 4）缩小目标差异，将目标通过类似生成对抗网络（Generative Adversarial 

Net，GAN[）\[24\]](#_page67_x87.00_y450.92)来提高小目标在图片中的分辨率或者将小目标生成为大目标尺度， 使得网络可以像处理大目标一样处理“放大”后的小目标，将图像中不同大小的 目标差异缩小，达到提高检测精度的目的。例如，Li 等人[\[25\]](#_page67_x87.00_y504.92)提出一种感知生成 对抗网络模型（Perceptual  GAN），通过缩小大物体与小物体之间的表示差异， 提高小目标检测的性能，生成器通过学习将小物体的低分辨率和噪声表示转换为 真实大小物体相似的超分辨率表示以达到欺骗判别器的目的，同时对生成器施加 生成的小目标表示必须有利于检测的约束。Bai等人[\[26\]](#_page67_x87.00_y558.92)设计了一种端到端的多任 务生成对抗模型，用于解决小目标检测问题，网络由一个超分辨率网络作为生成 器和一个多任务网络作为判别器组成，生成器能够在细小的模糊图像中进行细粒 度的上采样来还原细节信息以更加精确的进行目标检测，判别器可以对每个超分 辨率图像块给出真假得分、目标类别得分和边界框回归偏移量，但该方法在背景 与目标相似的情况下无法进行有效的区分检测。Huang等人[\[27\]](#_page67_x87.00_y612.92)将生成对抗网络引 入 Faster-RCNN结构中以提升小目标交通标志目标检测效果，以生成对抗网络方 式将小目标特征信息逼近大目标特征信息后进行识别以提高识别率（如图 1-7所 示），两个模型的结合使得网络复杂度提升，该方法实时性和模型计算量使得方 法无法部署在边缘设备。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.011.jpeg)

图 1-7 基于 GAN的小目标检测方法原理图 

2. 行为识别研究现状<a name="_page13_x87.00_y590.92"></a> 

行为识别在机器视觉领域也是热点之一，它通过对目标视频进行详细观测， 来确定行为人所做出的动作含义。图 1-8展示了一些行为识别视频帧中的不同动 作类型。与目标检测发展类似，在行为识别的研究初期，行为识别通常面临以下 困难：（1）特征提取难，行为特征往往受到光照、背景、遮挡等干扰因素的影响， 不够鲁棒和稳定；（2）行为分类的复杂性，人类行为是由多个基础的原子动作组 合而成，不同的人在做相同的动作时会有很大的差异，如振幅、角度、速度等， 而且行为之间没有明确的边界，很难划分和识别；（3）真实环境的多样性，人类 行为识别的最终目标是实际应用，但在实际应用中，环境是多变的，如照明条件 变化、快速运动导致抖动、物体遮挡、多个视角等，这些因素使视频质量不理想， 影响识别效果。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.012.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.013.png)

- a）打篮球行为                        （b）赛马行为                       

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.014.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.015.png)

- c）刮胡子行为                        （d）打鼓行为                       

图 1-8 行为识别视频帧示例图 

因此研究人员通常使用梯度直方图（Histogram of Oriented Gradient，HOG） 与尺度不变特征变换（Scale-Invariant Feature Transform，SIFT）作为二维图像特 征拓展到三维时空领域对行为进行识别的基础。例如，2011年 Shao等人引入基 于变换的描述符到人类行为识别领域同时提出一种新的基于变换的时空描述符， 将经典的二维变换方法应用于三维视频序列从而提取出时空特征，但 Shao等人 [\[28\]](#_page67_x87.00_y666.92)并未考虑到视频序列中可能存在的背景干扰、光照变化、遮挡等因素，这些 因素可能影响到描述符的提取和匹配。2014年 Ji等人[\[29\]](#_page67_x87.00_y703.92)提出一种基于改进的时 空特征人体动作识别方法，该方法利用兴趣点的空间分布和时序变化信息，可以 有效地处理旋转、尺度变换、相机运动、复杂背景、阴影等影响动作识别的因素， 提高了识别率和鲁棒性。之后研究员发现光流信息可以被用于描述视频特征的变 化，且能更直接的反映出视频中存在的动作信息，使得光流信息一度被广泛用于 行为识别领域，Thomas等人[\[30\]](#_page68_x87.00_y72.92)提出一种基于变形理论的光流估计方法，该方法 能够在不同尺度上处理大幅度的运动和非线性亮度变化，同时保持高精度和鲁棒 性，但该方法的计算复杂度较高，需要优化算法和实现技术来提高效率。同时另 有研究员提出一种将光流直方图特征、HOG特征、轨迹特征以及光流梯度直方 图特征共同组合成密集轨迹特征的方法，大大提高了当时行为识别的准确度。例 如，Jiang等人[\[31\]](#_page68_x87.00_y109.92)提出一种基于轨迹的人体行为识别方法，利用场景中的运动参 考点（Motion Reference Points）来校准每个轨迹的运动，并捕捉运动对象之间的 关系，将每个轨迹视为一个局部运动参考点，用于运动特征描述，从而得到一个

丰富的表示，包含了全局和局部的运动信息，但该方法需要对视频进行预处理， 提取轨迹和光流信息，这可能会增加计算成本和时间。还有 Yi等人[\[32\]](#_page68_x87.00_y146.92)同样采用 显著轨迹的人类动作识别方法，首先计算并结合两种轨迹显著性值，即外观显著 性和运动显著性，来捕捉静态和动态的视觉注意力信息，通过核直方图来描述显 著轨迹的特征，并利用支持向量机进行动作分类，因此有效的提取和表示显著性 轨迹对该方法而言至关重要。这些一度被广泛认为是最优秀的人工特征提取算法 [33,34]主要通过滑窗操作细化完整视频为一系列子视频后进行提取特征，再使用分 类器对特征进行分类，最后分析输出确切的行为识别结果。图 1-9展示了基于光 流的行为识别方法示意图。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.016.png)

图 1-9 基于光流的行为识别方法示意图 

随着深度学习的概念被提出，越来越多行为识别方法使用这一概念，与使用 传统方式的行为识别算法相比，深度学习能够从大量的视频数据中自动学习有效 的特征表示，能够归类出比手工特征提取方法更全面、更具特点的图像特征，从 而进一步提高识别的准确性。例如，Zhang等人[\[35\]](#_page68_x87.00_y274.92)提出一种基于改进的三维卷积 神经网络的行为识别方法，该方法利用光流和运动历史图像构建特征图像作为网 络输入，有效提取到时空特征，但该方法在复杂场景下的异常行为检测效果不理 想，以及对于不同摄像头视角下的异常行为识别能力不足。Zhang和 Li等人[\[36\] ](#_page68_x87.00_y328.92)使用双流卷积网络进行行为识别，双流网络结合 RGB图像与光流图像的空间和 时间特征捕获行为的外观与运动信息，与几种基准方法相比具有更高的准确性与 鲁棒性，但仍需提升微妙行为之间的区分准确率，权衡检测速度与精度的关系。 Hsueh和 Lie等人[\[37\]](#_page68_x87.00_y365.92)在卷积神经网络和长短期记忆（Long Short-Term Memory， LSTM）网络的基础上提出新颖的堆叠卷积自动编码图像聚类方法，配合两部摄 像机解决智能家居中出现的遮挡和轮廓模糊问题。同样，针对复杂场景中光照的 变化以及背景抖动的问题，Guan等人[\[38\]](#_page68_x87.00_y402.92)使用光流与运动历史图像组成的特征图 代替 RGB图像作为输入，配合 3D-CNN与 LSTM进行异常行为识别。为了进一 步提高识别的精确度，Byeon和 Kim等人[\[39\]](#_page68_x87.00_y439.92)提出一套“三流”方法，使得视频 中行为的时空特征、二维序列图像、三维视频和序列特征都被用于最终的识别输 出中。不仅是单人的行为识别，为解决拥挤场景下多人行为识别，提高算法实用

性，Fariba与 Mehran等人[\[40\]](#_page68_x87.00_y493.92)使用 AE（AutoEncoder）特征与 CNN、LSTM一起 产生较好的应用前景。尽管基于深度学习的方法在行为识别领域相比于传统方法 取得了更好的效果，但高度依赖物体和场景以及网络对时空信息描述不充分等问 题仍需等待学者不断探索解决方法。图 1-10展示了基于 3D卷积神经网络的行为 识别网络结构图。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.017.jpeg)

图 1-10 基于 3D卷积神经网络的行为识别网络结构图 

行为识别的本质是对同一物体在连续时间内做出的动作、空间的移动、产生 的形变做出的研判。早在 1973年，Johansson[\[41\]](#_page68_x87.00_y530.92)观察到只需要组合并追踪 10-12 个关节点便能形成对诸多行为例如奔跑、跳跃、行走的刻画，因此他提出人体运 动可以通过一些主要关节点的移动来进行描述。通过人体姿态估计得到的人体关 节点用于生成人体骨架，一个动作往往由多帧图像构成一个视频时间序列组成， 每隔一段时间由 K 个关节点坐标位置信息来表示相应人体骨架点，时域提前划 分序列输入，网络确定其所属的行为类型，从而完成一个行为识别周期。通过骨 架结构来估计行为不易受外界环境因素影响，且骨架信息明确简单，图 1-11展 示基于骨架的行为识别方法结果图。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.018.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.019.png)

图 1-11 基于骨架点的行为识别方法结果图 

3. 论文研究主要内容与目的<a name="_page17_x87.00_y76.92"></a> 

论文围绕海滩监控系统，重点开展海滩场景下的小目标游客检测算法研究以 及游客危险区域异常行为识别算法研究，最终设计出高适应度、正确率与效率兼 顾的小目标检测及其后续危险区域异常行为识别的海滩监控系统，通过部署在海 滩的监控自动捕获危险区域游客目标，帮助工作人员及时进行劝返并提防游客发 生诸如溺水等异常行为，本论文具体研究内容如下： 

- 1）针对复杂海滩场景模式下小目标游客检测精度不足的问题，论文提出

一种多层特征图信息融合的海滩小目标检测方法，从上下文信息与强化特征图信 息融合的角度提升小目标游客的检出率。首先，透过更全面、有效的 GAM注意 力机制思想结合 CSP结构提出 GCSAM结构，用于增强检测原 YOLOv5模型中 主干网络跨纬度感受区，聚焦小目标特征学习；其次，在颈部融合端使用 BIFPN 结构优化原 YOLOv5网络中 PANet结构，补全跨层特征信息之间的传递，使得 特征图包含更多的上下文信息；最后，采用幂变换改进原 YOLOv5 网络中 CIOU\_Loss为 Alpha-CIOU\_Loss，有效提升预测框的回归精度。自制海滩监控小 目标游客图像集，通过网页录屏操作采集监控视频，间隔一定时间抽取监控视频 中一定数量的图像形成海滩小目标游客数据图像集用于网络训练与测试。 

- 2）针对海滩场景的异常行为识别，主要针对溺水行为中能被检测到的骨

架点信息缺失导致行为判断准确率较低的问题，论文提出一种融合改进 Alphapose的时空图卷积网络的溺水异常行为识别算法。在前一阶段危险区域小 目标检测到游客信息后，通过硬件驱动获得目标更为清晰的画面信息，从充分利 用人体特征点的角度出发。首先，采用 Alphapose方式优化原 HRNet方式提取 游客骨架点信息，增强对游客信息姿态估计能力；其次，使用 YOLO系检测方 法替代原 Alphapose网络中的 RCNN系检测方法，有效提高检测速度与精度；再 次，结合时间信息与空间信息利用所采集到的有限游客关节点信息构建图神经网 络；最后判断出游客是否处于溺水等异常行为。自制溺水异常行为动作视频集， 在国内外公开的视频网站爬取有关溺水的视频，使用剪辑软件裁剪其中的溺水片

段，形成溺水异常行为动作视频集用于网络训练与测试。 

- 3）实现一套智能海滩监控系统，此系统包括海滩摄像头接入、小目标游

客检测、危险区域目标游客行为识别、检测效果显示等功能，其中小目标检测与 异常行为识别为论文所提的多层特征图信息融合的海滩小目标检测方法以及融 合改进 Alphapose的时空图卷积网络的溺水异常行为识别算法，设计并选取相应 系统软、硬件环境以及系统整体流程，搭建相关人机交互界面并显示算法实际部 署下运行结果。 

4. 论文组织结构安排<a name="_page18_x87.00_y76.92"></a> 

论文主要针对应用于海滩监控系统下的小目标检测和行为识别算法及其系 统开展研究。各章节的主要研究内容如下： 

第一章为绪论。本章主要介绍论文开展工作的研究背景以及研究背后的意 义，阐述了当前有关小目标检测与行为识别的国内外研究现状，结合海滩部署场 景分析存在的难点与痛点从而引出论文的主要研究内容与目的。最后阐述论文组 织结构安排。 

第二章为相关技术与理论基础概述。本章介绍了深度学习中的卷积神经网络 的基本组成单元结构及其用途，以及详细阐述了论文后续构建小目标检测与异常 行为识别算法及其系统时所采用算法的原始思想与原始网络模型结构特点，包括 一阶段目标检测 YOLO算法与图卷积神经网络算法。 

第三章为多层特征图信息融合的海滩小目标检测算法研究。本章针对复杂海 滩场景模式下小目标游客检测精度不足的问题，提出一种多层特征图信息融合的 海滩小目标检测方法，从上下文信息与强化特征图信息融合的角度提升小目标游 客的检出率。本章详细阐述了该方法模型的细节与改进思路，通过自制数据集与 其他主流方法进行对比实验验证，证明本章方法在海滩小目标游客密集、遮挡、 目标更小的情况下具有更好的效果。 

第四章为基于时空图卷积网络的溺水异常行为识别算法研究。本章针对游客 发生溺水异常行为后可被观测到的信息不足导致检测精度低的问题，提出融合改 进 Alphapose的时空图卷积网络模型进行异常行为判断。首先介绍了有关姿态估 计标注格式，其次提升 Alphapose中的目标检测步骤精度达到提升骨架点估计效 果，再次分析 ST-GCN 网络构建图卷积与时卷积的操作流程并将改进后的 Alphapose嵌入 ST-GCN模型中，最后通过自制数据集进行改进后 Alphapose方 法融合时空图卷积网络行为识别与相关算法的对比实验，证明所提方法取得更好 的人体骨架点获取结果以及更好的溺水行为与非溺水行为的识别率。 

第五章为海滩监控系统的小目标检测和行为识别系统研究。本章从明确系统 需求导向出发，根据前几章所提方法设计并实现应用于海滩监控系统下的小目标 检测和行为识别算法及其系统，分析所需软、硬件环境与系统整体流程，采用图 形用户交互界面（Graphical User Interface，GUI）方式对系统各项功能进行详细 构建并介绍相关信息，最后对系统整体流程进行演示。 

最后阐述论文的总结与展望。总结归纳论文的研究创新内容，针对论文开展 工作过程中出现的一些不足与缺陷进行相应的分析并提出一些尚可实践的规划， 以及对相关领域未来的研究发展方向做出展望。 

<a name="_page19_x87.00_y78.92"></a>第二章 相关技术与理论基础概述 

1. 卷积神经网络<a name="_page19_x87.00_y128.92"></a> 

Hubel和 Wiesel[\[42\]](#_page68_x87.00_y567.92)首次提出了卷积神经网络（Convolutional Neural Networks， CNN）这个概念，卷积神经网络随着科学进步不断地发展，现如今卷积神经网 络已经在图像处理、视频处理、音频处理等诸多领域大放异彩。不论是早些时间 的 AlexNet[\[8\]](#_page66_x87.00_y381.92)、VggNe[t\[43\]](#_page68_x87.00_y604.92)、ResNet[\[44\]](#_page68_x87.00_y641.92)等较为简易的网络模型还是现阶段的 YOLO 系列[45-48]、RCNN系列[49-51]等较为复杂的网络模型，本质上都是由一系列卷积神 经网络的基本组成单元所构成。卷积神经网络基础构成如图 2-1所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.020.jpeg)

图 2-1 基本的卷积神经网络结构图 

1. 卷积层<a name="_page19_x87.00_y487.92"></a> 

在卷积神经网络中，最基础、最核心的单元是卷积层（Convolutional Layers， Conv），它用一系列大小相同功能不同的卷积核对输入信息做特征提取，卷积层 的两个主要特点分别是权值共享和局部连接，它们让卷积神经网络在图像深度学 习领域表现出色，它们可以在不损失关键信息的情况下降低模型参数量，使运算 更高效和简单。卷积层中的神经元只和上一层里一小片区域的神经元相互关联的 模式就是局部连接，这样就能够学习到图像局部的特征，比如边缘，角点等。因 为图像里距离较远的像素之间关联度较小，而相邻像素之间则有很多相似信息， 关联程度高。通过局部连接，一个神经元只要对图像的一部分进行感知即可，把 所有部分感知的信息按顺序拼凑起来，就能得到图像整体的信息。权值参数共享 是指对于不同区域的特征信息使用相同的卷积核抽取特征，原因在于图像的底层 信息是不会随着位置的改变而改变的，因此对于提取相同的特征使用同一组卷积 核可以大大减少卷积操作所需要用到的参数。需要注意的是：高层特征通常是全 局的位置相关特征，所以在高层处理时不同位置需要用不同的神经网络权重，此 时就需要引入局部全连接层和全连接层来完成该任务。 

卷积神经网络中的卷积层操作通过卷积核按照给定的移动距离（步长）在输 入数据上滑动，将对应元素相乘后求和得到下一层特征图所需要的值，图像中卷 积计算方法如公式（2-1）所示。其中，中心像素的坐标用（*i*，*j*）表示，下一层 特征图由 *g*表示，当前层的特征图由 *f*表示，卷积核用 *h*表示。同时为了避免输 入信息的边缘信息可能被忽略计算的情况，引入填充操作（Padding）对输入信 息边界进行处理，保证输入信息的每个元素都被利用，卷积操作如图 2-2所示。

*g*(*i*, *j*)= *m*=å1,*n*=1 *f* (*i*+*m*, *j*+*n*)*h*(*m*,*n*) 公式（2-1）

m=−1,n=−1

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.021.jpeg)

图 2-2 卷积层操作示意图 

上述卷积为最基本的卷积操作，随着场景应用的拓展与研究的不断深入，不 断涌现出许多诸如空洞卷积[\[52\]](#_page69_x87.00_y294.92)、可变形卷积[\[53\]](#_page69_x87.00_y331.92)、深度可分离卷积[\[54\]](#_page69_x87.00_y368.92)等不同功能 不同效果的卷积形式，这里不做过多赘述。 

2. 池化层<a name="_page20_x87.00_y538.92"></a> 

池化层（Pooling Layers）是缩小长、高方向上的空间运算，通常在卷积层中 会周期性地插入若干池化层，这一操作是很有必要的。池化层设计之初是仿照人 类视觉系统对观测对象进行抽象和降维的概念，其本质是通过下采样操作来实现 对卷积层特征值的筛选，以下好处是使用过池化层的神经网络所特有的：（1）在 一定程度上可以防止过拟合现象发生；（2）特征降维，池化等价于在空间范围内 做了维度约减，使模型能够提取更广泛的特征，基于此进一步减少下一层输入量 达到减少计算工作量和参数个数的目的；（3）特征不变性，池化操作让模型更加 关注特征图中是否使模型更多地注意到特征映射中有无某特征，而不是特征的具

体定位。假设池化层是 *l*层，以 Pooling（**·**）表示池化函数，第 *l*层第 *j*个特征图

的权重用 *wjl*表示，*bj*表示偏置项，则池化操作如公式（2-2）。 

*l*

*x l* = *f* (*w l* ´Pooling(*x l*−1)+*b l*) 公式（2-2）

*j j j j*

特征图中部分区域的最大值、平均值或者随机矩阵选择的某个特征值，分别 对应最大池化、平均池化和随机池化这三种常用的池化操作。平均池化与最大池 化的含义可由其名称推测，选择特征图某些区域的平均值或最大值为结果来表示 此区域的池化操作，而随机池化则是通过随机方式来选取该区域池化后的输出， 具体操作如图 2-3所示。 

最大池化 6 8![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.022.png)

3 4



|1|1|2|4|
| - | - | - | - |
|5|6|7|8|
|3|2|1|0|
|1|2|3|4|

3\.25 5.25

平均池化

2 2



|0\.15|0\.35|0\.20|0\.40|
| - | - | - | - |
|0\.45|0\.05|0\.25|0\.15|
|0\.20|0\.20|0\.10|0\.40|
|0\.20|0\.40|0\.30|0\.20|

池化核大小：2\*2 步长：2

随机 5 4 池化 2 0

概率 矩阵

图 2-3 不同池化操作示意图 

3. 全连接层<a name="_page21_x87.00_y482.92"></a> 

卷积神经网络的尾部组成被称为神经网络中的全连接层（Fully  Connected Layers，FC），起到帮助神经网络完成最后的分类作用。全连接层将卷积层、池 化层等操作得到的原始数据映射成“分布式特征表示”，然后在样本标记空间中 对这些丰富的特征进行标记。全连接层本质上是对前层输出的特征进行加权求 和，*wi*是全连接层中的权重系数，*xi*是上一层第 *i*个神经元的值，*bi*是全连接层 的偏置项，计算方法如公式（2-3）。 

*y* (*x*)= *f* æçå*n w x* +*b* ö÷  公式（2-3）

*w*,*b* è *i i i i* ø

通常，若全连接层的上一层为全连接层时，则可将其转化为卷积核大小为 1

- 1的卷积操作，表明全连接层可通过卷积操作来实现；若全连接层的上一层为 卷积层时，则可将其转化为卷积核尺寸等于上一卷积输出结果高低宽度的全局卷 积。全连接层结构示意图如图 2-4所示，全连接层结构通常包括多个神经元，每 个神经元都与前一层的所有神经元相连。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.023.png)

图 2-4 全连接层结构示意图 

4. 激活函数<a name="_page22_x87.00_y270.92"></a> 

卷积层与池化层这些线性操作无论使用多少次，最后的模型依旧是线性的。 但人类实际生活中的诸多问题都不是简单的线性模型，线性模型的表达能力不足 解决这些问题，因此在卷积层与池化层过度时引入激活函数的作用就是增加非线 性因素，使得模型在处理实际问题时可以更加灵活。不引入激活函数的线性模型 与引入激活函数的非线性模型对比如图 2-5所示。常用的激活函数有 Sigmoid、 Tanh、ReLU等。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.024.jpeg)

- a）线性模型                      （b）非线性模型                       

图 2-5 线性模型与非线性模型对比图 

2. YOLO<a name="_page22_x87.00_y628.92"></a>算法基本原理 
1. YOLO<a name="_page22_x87.00_y684.92"></a>算法思想 

You Only Look Once（YOLO）是当前目标检测最先进的实时目标检测算法 之一，首次由 Joseph和 Santosh等人[\[45\]](#_page68_x87.00_y695.92)在 2016年提出，后经过不断完善改进，

现如今已经发展出了多个不同版本的 YOLO检测器[46-48]，但本质上算法的基本 思想都是一致的。 

YOLO算法以 R-CNN系列算法[49-51]为基础，摒弃了 R-CNN此类算法先通 过选择性搜索提取出 2000个左右的预选框并固定为统一大小，再交由 CNN网 络提取特征信息，最后进行边界框的回归以及分类器给出检测结果的思路。 YOLO选择将目标检测看作回归任务，采用卷积神经网络直接对目标类别和边界 框进行预测，利用一个全连接层同时实现分类和定位，相比于二阶段 R-CNN类 算法，YOLO算法可以大大加速检测过程，在牺牲较少精度的前提下取得惊人的 检测速度，YOLO算法检测原理如图 2-6所示。首先将输入的图像信息划分成 S

- S个单元格，通过每个格子所在位置与对应内容为基础，对于每一个网格都会 预测边界框的置信度以及 B个边界框（Bounding Box），边界框中包含经过归一 化处理的目标中心位置（B*x*，B*y*）与相对对应网格位置的偏移宽度 B*w*和偏移高 度 B*h*，置信度包含边界框中是否出现目标的概率 Po∈{0, 1}以及包含物体情况下 位置的准确性P \*IOUtruth；然后 C个类别的概率同样由每一个网格预测给出，经

o pred

过独热向量编码（One-hot  Encoding）确定最终输出的是哪一种目标类别；最后 经过非极大值抑制（Non Maximum Suppression，NMS）剔除重复的边界框，找 到置信度最佳的边界框。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.025.jpeg)

图 2-6 YOLO算法检测原理图 

2. YOLO<a name="_page23_x87.00_y611.92"></a>算法模型结构 

最基本的 YOLO算法模型结构如图 2-7所示。输入图像的尺寸为 448×448， 整个网络主体由 24个卷积层和 2个全连接层组成，最后在重塑（Reshape）操作 下，输出特征为 7×7×30。YOLO主要依靠建立一个 CNN网络来预测 7×7× 1024的张量（Tensor），然后使用两个全连接层执行线性回归将具有高置信度得 分的结果作为最终预测。通常在 3×3 的卷积后面会跟一个通道数更少的 1×1 的卷积，这种方式利用 1×1卷积的特性达到降低模型的计算量与提升模型的非

线性能力的目的。在激活函数的选择上，主要采用 Leaky ReLU[\[55\]](#_page69_x87.00_y422.92)作为网络激活 函数，而最后一层改用线性激活函数，同时为防止过拟合的发生，在训练中使用 Dropout与数据增强操作。损失函数方面，YOLO使用预测值和真实值（Ground Truth）之间的误差平方的求和（Mean Square Error，MSE）来计算损失。损失函 数包括三部分：边界框回归损失、目标物体损失以及预分类损失，具体表达式如 公式（2-4）。其中检测层个数由 N表示，标签分配到先验框的目标个数由 B表 示，该尺度被分割成的网格数为 S×S。边界框回归损失由 *L*box表示，对每个目 标单独计算；目标物体损失由 *L*obj表示，对每个网格单独计算；分类损失由 *L*cls 表示，同样对每个目标单独计算，这三种损失函数相应的权重分别由 *ω*1、*ω*2、*ω*3 对应表示。 

- *B* ö

Loss =å*N* çw *å*Bi L* +w * *S*å*i***Si L* +w *å*i L* ÷ 公式（2-4）

*i* è 1 *j boxj* 2 *j objj* 3 *j clsj* ø![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.026.png)

**7 ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.027.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.028.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.029.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.030.png)**

**448 ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.031.png)**

**7** 

**112 ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.032.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.033.png)**

**3 ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.034.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.035.png)**

**3 56**

**33 28 3 14 3 7 3 7**

**112 56 283 14 3 7**

**3**

**448 3 192 256 512 1024 7 7**

**Conv.Layer Conv.Layer Conv.Laye10r24**

**C7on\*7\*v.L64aye-s-2r Con3\*v.L3\*1aye92 r 3\*3\*256 3\*3\*512 \*4 31\*\*31\*\*1501224 \*2Co3\*n3v.L\*10aye24r1024Conv.Laye4r096Conv.Laye7r 30![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.036.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.037.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.032.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.038.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.032.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.039.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.032.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.040.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.032.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.041.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.042.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.043.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.044.png)**

**1\*1\*128 1\*1\*256![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.045.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.046.png)**

**Maxpool Layer Maxpool Layer 3\*3\*1024**

**2\*2-s-2 2\*2-s-2 1\*1\*256 1\*1\*512 3\*3\*1024**

**3\*3\*512 3\*3\*1024 3\*3\*1024-s-2**

**Maxpool Layer Maxpool Layer**

**2\*2-s-2 2\*2-s-2**

图 2-7 YOLO算法基本模型结构图 

3. 图卷积网络基本原理<a name="_page24_x87.00_y535.92"></a> 
1. 图卷积网络思想<a name="_page24_x87.00_y591.92"></a> 

随着信息化时代的不断发展，人类社会无时无刻都在产生各式各样的数据， 这些数据类型从大体上可以分为两类：其中一类是以文本、图像为代表的欧几里 德数据，另一类则以知识图、人体骨架信息为代表的非欧几里德数据。非欧几里 德数据可以用图（Graph）结构进行表示，在图结构中主要由节点与连边构成即 G={V，E}，V={v1，v2，…，vn-1，vn}代表构成图结构的所有节点，E={e1，e2，…， em-1，em}代表节点与节点之间是否存在相关性的连边，节点之间的边（v*i*，v*j*）

∈E构成邻接矩阵 **A**∈**ℝ**N×N，若此时连边具有指向性则称图为有向图，反之为无 向图，图结构示例如图 2-8所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.047.jpeg)

- a）无向图                  （b）有向图                 （c）邻接矩阵            

图 2-8 图结构示例图 

卷积神经网络依靠图像此类欧几里德数据在域中的平移不变性可以很轻松 地提取出数据之间的特征关系，但面对人体骨架信息此类非欧几里德数据之间存 在独有联系的情况则束手无策。为解决 CNN等深度学习方法在处理图结构的局 限性，Bruna等人[\[56\]](#_page69_x87.00_y459.92)首次提出图卷积神经网络（Graph Convolutional Networks， GCN），后在 Thomas与 Ma[x\[57\]](#_page69_x87.00_y496.92)的研究下进一步提升了检测速度与精度。图卷积 神经网络同样是一种特征提取器，其拓展了卷积神经网络处理低维图像数据到任 意图结构化所表示的高维空间。 

GCN 作为一种神经网络，其核心层与层之间的传播方式如公式（2-5）。**H** 表示网络图的节点特征矩阵，σ（**·**）表示对应层的激活函数，**W**是每层的权重矩 阵，**A** 表示图网络结构的邻接矩阵，**Â**=**A**+**I** 是带有自连接网络的邻接矩阵，其 中 I表示单位矩阵，**Dˆ**代表 **Â**的度矩阵，度矩阵是只有斜对角元素有值的对角矩

阵，即**D**ˆ =å**A**ˆ 。为避免邻接矩阵 **A**出现对角线元素均为 0的情况，此时 **A**和节 ii ij

点特征矩阵j **H**相乘后会导致该节点自身的特征被忽略，所以采用 **Â**进行特征提 取，而 **Â** 能将节点自身的特征保留下来，并且通过网络传递下去。进一步为避 免某些节点连边过多所导致计算后参数过大的情况，需要对节点归一化操作

**1 1**

**Dˆ** − **AˆDˆ** −，同时使用归一化或者标准化来将所有数据限制在一个合理的区间范围

**2 2** 内，可以消除奇异样本数据导致的不良影响，否则未经过归一化的数据意味着特

征指标之间的量纲影响没有被消除，会改变特征的原本的分布情况，会极大地影 响最终的效果。 

**H**l+1 = s(**D**ˆ −12**A**ˆ**D**ˆ −2**H**l**W**l)  公式（2-5）

1

2. 图卷积网络模型结构<a name="_page26_x87.00_y78.92"></a> 

图卷积神经网络模型结构如图 2-9所示。*X*代表每个节点的特征，*C*代表初 始特征的维度数量，*Z*是得到新的节点特征，*F*为输出特征维度的数量。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.048.jpeg)

图 2-9 图卷积神经网络模型结构示意图 

图卷积操作类似于卷积神经网络中的卷积概念，通过构建邻域矩阵来实现对 邻域中的节点与卷积核参数的内积，但是图结构无法保证数据之间存在固定的邻 域关系，对此 GCN采用随机移动的方式，通过随机化被选中概率的期望大小， 选择固定量的邻域节点，然后根据节点在不同情况下得到的期望概率对邻域内的 节点进行排序，排序完成后的节点通过卷积核完成卷积操作，并更新节点信息， 再通过池化层与激活函数，不断重复直至达到预期目标。最后节点在每个类别上 的概率是通过全连接层后使用分类器（比如 SoftMax）或直接使用分类器来获得， 从输入输出可以看出不论经过多少层卷积、池化、激活等变换操作，输入节点之 间的联系并没有发生过改变，即图的结构从始至终都是一样的。 

4. 本章小结<a name="_page26_x87.00_y528.92"></a> 

本章所介绍的相关技术与理论基础概述，主要包括卷积神经网络、YOLO算 法、图卷积神经网络。卷积神经网络主要介绍了构成卷积神经网络的各个结构， 包括卷积层的结构与功能、池化层的结构与功能、全连接层的结构与功能以及激 活函数的功能与形式。YOLO算法方面介绍了其检测算法的基本思想，阐释其基 本网络模型结构，图卷积神经网络从卷积无法处理图结构数据出发，引出图卷积 基本思路，解释其传播方法以及构建原理，并通过图卷积神经网络模型结构进一 步介绍图卷积的数据处理过程。本章对相关技术理论基础的解释为后续应用于海 滩监控系统的小目标检测和行为识别算法及其系统研究奠定充足的理论基础。

<a name="_page27_x87.00_y78.92"></a>第三章 多层特征图信息融合的海滩小目标检测算法

研究 

1. YOLOv5<a name="_page27_x87.00_y148.92"></a>网络结构分析 

YOLOv5作为当前检测领域检测性能卓越、工程部署率较高的目标检测模型 之一，网络结构主要由四部分组成：输入端、主干网络端、颈部融合端以及预测 输出端，但该网络结构针对海滩小目标游客检测仍然存在一些不足。输入端将数 据送入模型进行检测。主干网络端是整个检测网络的核心部分，主要部分采用跨 阶段局部（Cross Stage Partial，CSP）结构，这是一种能够减少计算和内存消耗 同时提高卷积神经网络特征学习能力的新型主干网络，但由于 CSP与卷积结构 交替堆叠，使得主干网络端对小目标的特征提取能力欠佳。颈部融合端采取路径 聚合网络（Path  Aggregation  Network，PAN）结构与特征金字塔网络（Feature Pyramid Network，FPN）结构相结合，FPN对特征图进行上采样并与主干网络端 相同大小的特征图交互上下文信息；其次，PAN对 FPN得到的上采样特征图进 行下采样操作并与 FPN各部分特征图交互语义信息；最后，得到多尺度特征图 送入预测输出端。这种结构可以有效地增强多尺度特征图的语义表达和目标定位 能力，但由于其结构为单层单向信息传递方式，因此会造成一定的海滩游客特征 信息损失，从而降低检测精度。预测输出端采用完全交并比损失（Complete Intersection Over Union Loss，CIOU\_Loss）作为损失函数并结合非极大值抑制用 于处理因遮挡导致检测准确率下降的问题，但其仍会受到噪声等因素影响造成定 位偏差从而降低检测精度，无法适应海滩监控设备所采集的图像噪声。 

2. 多层特征图信息融合的海滩小目标检测算法模型<a name="_page27_x87.00_y532.92"></a> 

改进后的 YOLOv5网络结构可以更好地适应海滩场景下小目标游客检测， 网络整体结构如图 3-1所示。针对上述问题，为更好的满足海滩小目标游客检测 场景需求，兼顾小目标游客检测的精确性与实时性，本课题从利用上下文信息以 及强化特征图信息融合[58-60]的角度出发在原 YOLOv5网络的基础上进行改进提 出多层特征图信息融合（Feature Map Information Fusion YOLO，FMIF-YOLO） 小目标检测方法，保证实时性前提下提高海滩小目标游客的检出率。在主干网络 端与颈部融合端根据全注意力机制（Global Attention Mechanism，GAM）思想[\[61\] ](#_page69_x87.00_y661.92)提出一种全局跨阶段注意力结构（Global  Cross  Stage  Attention  Mechanism，

GCSAM），使得网络更好地聚焦需要关注的小目标特征；再将 PAN+FPN结构调 整为双向特征金字塔网络（Bidirectional Feature Pyramid Network，BIFPN）结构， 此操作位于原 YOLOv5网络颈部融合端，网络进一步融合多尺度特征图上下文 信息，使得送入输出预测端的小目标特征信息得到最大程度的保留；最后采用 Alpha-CIOU\_Loss优化原 YOLOv5网络预测输出端中的 CIOU\_Loss使检测框定 位回归更加准确，解决海滩监控采集图像中噪声所带来的定位精度损失问题。 



<table><tr><th colspan="1" rowspan="2"></th><th colspan="1" rowspan="2"></th><th colspan="1" rowspan="2" valign="top">![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.049.png)</th><th colspan="2" valign="top">Conv</th></tr>
<tr><td colspan="2" valign="bottom">CBL</td></tr>
<tr><td colspan="3" rowspan="13" valign="bottom"><b>输入端</b>   </td><td colspan="2" valign="bottom">CSP1_X</td></tr>
<tr><td colspan="2" valign="bottom">CBL</td></tr>
<tr><td colspan="2" valign="bottom">GCSAM1</td></tr>
<tr><td colspan="2"></td></tr>
<tr><td colspan="2" valign="top">CBL</td></tr>
<tr><td colspan="2" valign="top">GCSAM1</td></tr>
<tr><td colspan="2"></td></tr>
<tr><td colspan="2" valign="top">CBL</td></tr>
<tr><td colspan="2" valign="bottom">GCSAM1</td></tr>
<tr><td colspan="2"></td></tr>
<tr><td colspan="2" valign="bottom">SPPF</td></tr>
<tr><td colspan="1" rowspan="2"></td><td colspan="2"></td></tr>
<tr><td colspan="3"></td></tr>
</table>

<table><tr><th colspan="7"></th><th colspan="8" rowspan="1">Concat</th><th colspan="3" rowspan="2"></th><th colspan="5" rowspan="4"></th><th colspan="3" rowspan="4"></th><th colspan="1" rowspan="4"></th></tr>
<tr><td colspan="3">CBL</td><td colspan="4"></td></tr>
<tr><td colspan="1"></td><td colspan="2"></td><td colspan="4" rowspan="2"></td></tr>
<tr><td colspan="3" rowspan="2" valign="bottom">Upsample</td></tr>
<tr><td colspan="1" rowspan="3"></td><td colspan="1" rowspan="3"></td><td colspan="4" rowspan="3"></td><td colspan="4" rowspan="3"></td><td colspan="6" rowspan="3"></td></tr>
<tr><td colspan="3"></td></tr>
<tr><td colspan="3" valign="bottom">Concat</td></tr>
<tr><td colspan="3"></td><td colspan="4"></td><td colspan="4" rowspan="2"></td><td colspan="5" rowspan="2"></td><td colspan="5" rowspan="2"></td></tr>
<tr><td colspan="3" valign="bottom">CSP2_X</td><td colspan="4"></td></tr>
<tr><td colspan="3" valign="top">CBL</td><td colspan="3" rowspan="1"></td><td colspan="2" rowspan="4"></td><td colspan="4" rowspan="3"></td><td colspan="5" rowspan="3"></td><td colspan="5" rowspan="3"></td><td colspan="3" rowspan="3"></td></tr>
<tr><td colspan="1"></td><td colspan="2"></td></tr>
<tr><td colspan="3" valign="bottom">Upsample</td></tr>
<tr><td colspan="2"></td><td colspan="1"></td><td colspan="4"></td><td colspan="4"></td><td colspan="6"></td><td colspan="4"></td></tr>
</table>
CBL Conv BN LeakyRelu ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.050.png)CSP1\_X

CBL Res unit Conv Concat

BN LeakyRelu CBL Conv

X**个**

GCSAM1 CSP1\_X GAM SPPF CBL

CBL

Max Max Max

Pool Pool Pool Concat CBL

CSP2\_X

CBL CBL Conv Conv

Concat BN LeakyRelu CBL CBL

2X**个**

GCSAM2 GAM

CSP2\_X

图 3-1 FMIF-YOLO网络整体结构图

1. 主干网络端优化<a name="_page28_x87.00_y398.92"></a> 

考虑到从海滩监控获取到的海滩游客目标小、信息少的特点，强化已知信息 的提取至关重要，注意力机制通过挖掘原始数据中的相关性，过滤无关信息并强 化关键特征信息的提取能力。因此，本课题引入注意力机制思想解决原 YOLOv5 主干网络的 CSP与卷积交叠结构提取小目标游客特征信息能力不足的问题。 

注意力机制仿效人类处理信息时会自动关注感兴趣部分的特点，在自然语言 处理领域运用广泛。文本注意力机制通过对重要单词加权，实现保持句子语义不 变的特性。类比一维的文本信息，二维图像中同样可以施加权重信息，使得重要 的图像信息得以保留。本课题提出 GCSAM结构如图 3-2所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.051.jpeg)

图 3-2 GCSAM模块结构图 

GCSAM模块通过增加 YOLOv5主干网络端跨纬度的感受区，针对小目标 信息进行更加细致提取，采用在尽可能减少信息弥散的情况下放大全局维度交互 特征的设计思路，相比于如今主流的小目标检测注意力机制更好地从通道、空间 宽度和空间高度三个维度整合所有的重要特征，增强跨维度的信息提取能力，提 升抑制非重要像素信息的效率。对于一个给定的特征图 F1∈ℝC×H×W，通道注意 力模块及空间注意力模块分别生成相应的特征图 F2=Mc（F1）UF1、F3=Ms（F2） UF2，Mc与 Ms分别表示通道注意力操作及空间注意力操作。相比于卷积块注意 力模块（Convolutional Block Attention Module，CBAM[）\[62\]](#_page69_x87.00_y698.92)在三维通道、空间高 度、空间宽度之间只选取两个方向进行注意力权重施加，忽略三个方向间的相互 作用，导致丢失跨维信息（CBAM通道注意力结构如图 3-3所示）。GCSAM在 通道注意力操作上直接采用 3维排列（Permutation）的方式保留三个方向上的特 征信息，通过多层感知机放大跨维通道及空间的依赖性。相比于挤压-激发

- Squeeze-and-Excitation layer，SElayer[）\[63\]](#_page70_x87.00_y72.92)注意力模块通过池化操作造成非重要 像素信息抑制失败导致效率不佳的问题（SElayer空间注意力结构如图3-4所示）， GCSAM 在空间注意力操作上通过删除挤压池化操作来保留空间特性映射获得 更多的空间信息，再通过两个卷积层进行空间信息融合，达到提高抑制非重要像 素信息的效率。 

MaxPool ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.052.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.053.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.054.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.055.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.056.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.057.png)

Sigmoid![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.058.png)

**通道注意 特征图![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.059.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.060.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.061.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.062.png)**F AvgPool **力**M )![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.063.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.064.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.065.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.066.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.067.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.068.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.069.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.070.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.071.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.072.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.073.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.072.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.072.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.072.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.074.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.074.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.075.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.076.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.077.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.078.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.079.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.080.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.081.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.082.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.083.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.073.png)

(F

1 c 1

**多层感知机**

图 3-3 CBAM通道注意力结构图 

特征图**F AvgPool Linear Relu Linear Sigmoid** 空间注意力 ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.084.png)**2 Ms(F2)**

图 3-4 SElayer空间注意力结构图 

最后本课题不仅将 GAM思想与主干网络端相结合，而且将其与颈部融合端 相结合（如图 3-1中 GCSAM2模块所示，融合特征图经过 CSP2\_X模块后，再 进入 GAM模块，最后通过卷积模块进行维度调整），一方面使得特征图更加关 注需要被检测的小目标，另一方面可以有效忽略特征融合后特征图中不需要被重 点关注的区域，便于后续预测输出层进行游客目标及其位置的预测。 

2. 颈部融合端优化<a name="_page29_x87.00_y674.92"></a> 

原 YOLOv5在颈部融合端使用了 FPN+PAN结构[\[64\]](#_page70_x87.00_y126.92)，在多个尺度上整合了 强语义特征和强定位特征。FPN 自顶向下传递高层次的语义信息，PAN自底向 上传递低层次的位置信息，从而提升不同检测层的参数效果。尽管 FPN+PAN结 构作为当前较为优秀的多尺度特征融合方法，但其结构为单层单向信息传递。由 于特征图在下采样过程中必定存在信息的丢失，单层传递的方式必然会缺失部分 跨层的语义特征与定位特征。对此本课题引入 BIFPN结构[\[65\]](#_page70_x87.00_y180.92)进入原 YOLOv5的 颈部融合端（具体结构见图 3-1颈部融合端）中调整 FPN+PAN结构。FPN+PAN 结构与BIFPN结构对比示意图如图3-5所示。以**P**6层输出特征图为例，FPN+PAN

的计算公式为公式（3-1），而 BIFPN的计算公式为公式（3-2）、公式（3-3）。 **P**6*out* =Conv(**P**6*in* +Resize(**P**7*in*)+Resize(**P**5*out*)) 公式（3-1）

**P** *td* =Convç 1 6*in* 2 ( 7 )÷ 公式（3-2）

æ*w* ×**P** +*w* ×Resize **P** *in* ö

6 ç *w* +*w* +e ÷

- 1 2 ø

æç*w* × **P**6 + 3 Resize(**P**5*out*)ö÷

**P** =Convç 1' **P**6*in* +*w*2'× *wtd* +*w* ' +e ÷ 公式（3-3）

*out*

6 *w* ' + ' *w* '

- 1 2 3 ø

**P** in表示第 X层的输入特征图，**P** td表示第 X层的中间特征图，**P***x*out表示第

*x x*

X层的输出特征图，*wi*表示针对融合的各个尺度特征图增加一个权重信息调节每 个尺度的贡献度，*ε*用于防止数据溢出。FPN+PAN结构中 **P**6层仅仅将不同尺寸 的特征图调整为相同的大小后求和，实际中不同分辨率的输入特征对输出端的作 用是不相等的。而 BIFPN结构采用跨尺度连接的方式，最后送入预测端的特征 图针对不同输入特征设置可调节的权重信息，让网络自适应学习调整权重。 BIFPN 促进不同层之间的特征融合，使得最后送给预测输出端的特征图包含更 多的上下文信息，可以更好地帮助小目标海滩游客传递语义及定位信息。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.085.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.086.png)

- a）FPN+PAN结构图                 （b）BIFPN结构图                     

图 3-5 FPN+PAN结构与 BIFPN结构对比图 

3. 损失函数优化<a name="_page31_x87.00_y78.92"></a> 

边界框的定位回归关乎到最终能不能精确给出目标检测结果。现阶段目标检 测中都采用交并比（Intersection Over Union，IOU）的方式代替传统 n范数方法 描述网络目标预测边界框和真实边界框的位置关系，该方法相比于 n范数方法具 有尺寸不变性，鲁棒性更佳。IOU表达式为 IOU=（A∩B）/（A∪B），A表示 网络预测所产生的候选框，B表示预先通过人工标记的真实框。使用 IOU\_Loss 作为定位损失反应真实框与预测框的关系即 IOU\_Loss=1-IOU。由于 IOU\_Loss 无法描述重叠样本上的梯度消失、预测框在真实框内部以及预测中心点相同但横 纵比不同等问题导致定位关系无法准确表述。原 YOLOv5 预测输出端采用 CIOU\_Loss 将重叠面积、中心点距离以及横纵比作为约束项加入损失函数中， 图 3-6为 CIOU\_Loss示意图，其表达式为公式（3-4），*b*、*b*gt表示预测框与真实 框的中心点坐标。*ρ*（**·**）表示计算两个框之间中心点的欧氏距离。*c*表示将预测 框与真实框完全封闭最小外接矩形的对接线距离，用于防止损失函数值过大，提 升收敛速度。*v*用于描述预测框与真实框横纵比的一致性，*β*为权重系数用于平 衡 *v*防止其失衡，*v*与 *β*的表达式为公式（3-5）、公式（3-6），*w*gt、*h*gt和 *w*、*h* 表示真实框与预测框的长宽。 

w![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.087.png)

b ρ(b, bgt) h

C bgt hgt

w gt

图 3-6 CIOU\_Loss示意图 

r2(*b*,*bgt*)

CIOU\_Loss =1−IOU +~~ +b*v* 公式（3-4）

*c*2

*v* = 4 (arctan *wgt* − *w* 2 公式（3-5）

arctan~~ )

p *hgt h*

b = *v*

(1−IOU)+*v* 公式（3-6）

尽管 CIOU\_Loss可以解决许多 IOU无法规避的问题，但 CIOU\_Loss在一些 小数据集和噪声情况下由于数据集或者图像质量的限制仍会出现预测框的回归 精度下降问题。原因在于不同目标间的相似性或者噪声引入带来的错误信息导致 有些目标 CIOU\_Loss 计算时正确的类别预测框与错误的类别预测框差距不明 显。为消除在海滩监控下可能出现的噪声以及图像质量所导致的检测率下降问题 同时避免引入过多算法使得模型复杂，本课题在 CIOU\_Loss的基础上引入幂变 换得到 Alpha-CIOU\_Loss[\[66\]](#_page70_x87.00_y234.92)，其表达式为公式（3-7）。 

r2a (*b*,*bgt*) a

Alpha-CIOU\_Loss =1−IOUa + 2a~~ +(b*v*) , a >0 公式（3-7）

*c*

幂变换约束项由 *α*所表示，由于幂变换并不会改变 CIOU\_Loss关键性质， 而且采用幂变换可以使得其他约束项随 IOU一同进行等幅度的单调变化，改变 *α* 的大小可以帮助模型关注更多高 IOU数值的候选框，放大相似信息之间的差异 性，提高网络定位和分类的性能。实验表明 0<*α*<1 时会导致降低最终性能，使 得高 IOU目标的数量减少，最终会产生更多定位较差的目标；*α*>1时由于会引 入一段绝对损失量，可以很好地优化所有检测层的候选目标，同时由于 *α* 对高 IOU数值候选框的绝对梯度变化，可以加快网络对这些候选框的学习。因此本课 题通过 Alpha-CIOU\_Loss 实现抗干扰能力达到对海滩小目标模型不同检测层预 测框的回归精度提升。 

3. 实验及结果分析<a name="_page32_x87.00_y468.92"></a> 
1. 数据集的建立与实验环境配置<a name="_page32_x87.00_y536.92"></a> 

由于当前海滩安全建设仍处于发展阶段，可用公开数据信息不足，本课题选 择在自建海滩监控场景下小目标游客数据集上验证本课题所提算法的有效性，同 时在 Visdrone 公开数据集辅助验证。数据集通过采集监控视频画面制作，图像 分辨率为 1920×1080，帧率为 25 fps，得到共计 1227张图像。从中选取 85%图 像作为训练集，15%图像作为测试数据集用于网络模型性能指标的评价。为了增 强模型的泛化能力，对小目标数据进行数据增强，通过在 YOLOv5输入端执行 若干数据增强操作处理原始数据集。利用图像标注工具 LabelImg进行人工标注， 生成“.xml”文件保存标注框坐标和目标类型等信息。部分标注过程及海滩监控 场景实验图如图 3-7所示。 

本课题实验配置环境如下：Ubantu18.04操作系统、CPU为Intel Core i9-8950、 GPU为 NVIDIA GeForce GTX 1080（8GB显存），内存 32GB。基于 Pytorch架 构使用 Python3.8环境和 GPU加速平台 CUDA  10.1实现本课题方法。根据硬件 性能，模型训练的参数设置如下：图像尺寸赋值为 640，迭代批量大小赋值为 4， 总迭代次数赋值为 200，初始学习率赋值为 0.0100。优化方式使用随机梯度下降， Momentum动量赋值为 0.9370，权重衰减系数赋值为 0.0005，Alpha-CIOU\_Loss 中 *α*根据实验测试结果取 3为最优。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.088.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.089.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.090.png)

- a）部分标记实验图一          （b）部分标记实验图二         （c）部分标记实验图三       

图 3-7 部分标注过程及海滩监控场景图 

2. 实验策略与评价指标<a name="_page33_x87.00_y358.92"></a> 

本课题针对海滩场景下小目标游客的检测任务采用查准率（Precision）、查 全率（Recall）、平均精度均值（mean Average Precision，mAP）以及每秒传输帧 数（Frames Per Second，FPS）等公共评价指标对本课题所提网络模型进行全面 评价，通过消融实验验证本课题所提各部分改进方法的有效性，通过对比实验验 证本课题所提方法相比于现有算法模型的优越性。本课题将海滩小目标游客归类 为正样本（Positive Sample），将其他背景信息归为负样本（Negative Sample）进 行实验并计算相应的实验评价指标。 

查准率（Precision）是指针对模型预测结果中预测为正样本的正确的比例， 在预测结果中正样本包含预测正确的正样本（True Positive，TP）以及错将负样 本预测为正样本（False Positive，FP）的误报，查准率越接近 1说明模型预测结 果中将背景信息错分为小目标游客的次数越少，模型的预测效果越好，其表达式 为公式（3-8）。 

TP

Precision = TP+FP´100% 公式（3-8）

查全率（Recall）表示训练样本中正确预测正样本占所有正样本的比例，根 据预测结果正样本被分为预测正确正样本（True Positive，TP）以及错将正样本 预测为负样本（False  Negative，FN）的漏报，查全率越接近 1说明模型将小目 标游客错分为背景信息次数越少，模型学习效果越好，其表达式为公式（3-9）。

TP

Recall= TP+FN´100% 公式（3-9）

平均精度均值（mean Average Precision，mAP）是用来衡量模型整体检测精 度的重要指标，由查准率与查全率绘制出的曲线（PR曲线）所围成面积来表示。 PR 曲线所围成的面积越大，平均精度均值越接近 1，说明模型整体检测效果越 好，其表达式为公式（3-10）。其中 N表示需要检测的所有类别数，本课题中 N=1， IOU阈值设置为模型评估通用的 0.5。 

åò1P(R)dR

mAP = 0~~ ´100% 公式（3-10）

N

每秒传输帧数（Frames Per Second，FPS）用于衡量模型处理图像的速度， 该指标受限于模型的计算量与实验过程中使用的硬件性能。一般地，认为该算法 模型满足实时性要求为检测速度不小于实时监控传输帧率（25 fps）即可。 

3. 消融实验<a name="_page34_x87.00_y338.92"></a> 

为验证本课题所提的方法对海滩小目标游客检测各个改进部分性能的有效 性，设计一组消融实验，对比分析了（1）原 YOLOv5模型（Baseline）；（2）将 Alpha-CIOU\_Loss损失函数单独用于原 YOLOv5模型；（3）将 BIFPN连接结构 单独用于原 YOLOv5模型；（4）将 GCSAM模块单独用于原 YOLOv5模型；（5） 同时将 Alpha-CIOU\_Loss损失函数与 BIFPN连接结构用于原 YOLOv5模型；（6） 同时将 Alpha-CIOU\_Loss损失函数与 GCSAM模块用于原 YOLOv5模型；（7） 同时将 GCSAM 模块与 BIFPN 连接结构用于原 YOLOv5 模型；（8）同时将 Alpha-CIOU\_Loss损失函数、BIFPN连接结构与 GCSAM模块用于原 YOLOv5 模型。所有消融实验具体实验结果如表 3-1所示。 

表 3-1 不同改进策略及其组合使用网络模型实验测试结果 



|网络结构 |Precision |Recall |mAP |FPS |
| - | - | - | - | - |
|YOLOv5 |89\.49% |76\.70% |83\.20% |41\.49 |
|YOLOv5+α-CIOULoss |86\.85% |78\.09% |83\.83% |42\.37 |
|YOLOv5+BIFPN |88\.38% |76\.76% |82\.70% |40\.00 |
|YOLOv5+GCSAM |90\.89% |78\.90% |86\.06% |28\.57 |
|YOLOv5+BIFPN+α-CIOULoss |87\.57% |78\.14% |84\.30% |**42.55** |
|YOLOv5+GCSAM+α-CIOULoss |90\.52% |81\.28% |87\.22% |28\.49 |
|YOLOv5+GCSAM+BIFPN |91\.02% |78\.98% |86\.12% |28\.24 |
|YOLOv5+GCSAM+BIFPN+α-CIOULoss |**91.49%** |**82.03%** |**87.56%** |28\.09 |

根据表中实验数据可以看出：本课题所提出的三部分改进策略在单独使用对 网络性能都有部分的提升，组合使用时更加优秀。当单独使用 Alpha-CIOU\_Loss 时相比于Baseline结构Recall提升1.39%，Precision下降2.64%，mAP提升0.63%， 说明 Alpha-CIOU\_Loss尽管可以有效降低将小目标游客错分为背景信息的情况， 提升小目标的漏检率，但由于未加入注意力机制前的网络特征提取信息能力不够 集中、细致，因此导致查准率出现下降，但从 mAP提升来看 Alpha-CIOU\_Loss 对整体网络效果是正向的，FPS提升进一步说明改进的损失函数计算方法可以加 快网络对这些候选框的学习，进一步提升精度。 

仅使用 BIFPN从客观评价指标来看除 Recall略微提升之外其余指标均出现 了下降，但当 BIFPN结构与其他模块组合使用时均比单独使用不含 BIFPN结构 的某一方法有进一步的提升。例如，Alpha-CIOU\_Loss结合 BIFPN时 Recall进 一步提升 0.05%，有效改善 Precision 下降，进一步提升整体模型检测效果， GCSAM结合 BIFPN模型检测效果同样进一步提升检测效果。BIFPN通过补全 跨层之间的特征信息融合来提升网络性能，由于原 YOLOv5作为当前十分优秀 的检测模型已经尽力提取特征信息导致跨层之间信息差异化不明显，使得 BIFPN 不但没有起到信息融合反而引入多余的计算量导致网络性能下降。 

采用 GCSAM结构很大程度提升原 YOLOv5的模型检测效果，Precision提 升 1.40%，Recall提升 2.20%，mAP提升 2.68%，充分说明本课题使用 GAM注 意力机制可以更进一步聚焦到海滩小目标游客信息提取上，使得网络可以更加精 确地关注和学习到小目标的特征，提高模型的检测效果，但也因此使得计算量大 幅提升，导致 FPS下降明显。尽管 FPS相比 Baseline下降 13.40 fps但依旧满足 实时检测要求。 

将 GCSAM结构结合 Alpha-CIOU\_Loss和 BIFPN所实现的本课题方法进一 步提升了在单独使用 GCSAM 情况下的模型检测性能，Precision 提升 2.00%， Recall提升 5.33%，mAP提升 4.36%。充分说明 GCSAM结构的引入所带来的小 目标特征图信息相比原 YOLOv5更加细致从而使得 BIFPN可以弥补更多跨层采 样所丢失的特征信息，而更细致的特征图信息包含更精确的定位信息促进 Alpha-CIOU\_Loss进行预测框更精准的回归。 

4. 算法性能测试及对比实验<a name="_page35_x87.00_y638.92"></a> 

为进一步验证本课题所提改进的 YOLOv5海滩小目标游客检测算法的有效 性，采用当前主流的小目标检测改进方法如：SSD方法[\[67\]](#_page70_x87.00_y271.92)、CBAM方法[\[62\]](#_page69_x87.00_y698.92)、SElayer 方法[\[63\]](#_page70_x87.00_y72.92)以及原始 YOLOv5与本课题方法进行客观指标比较，在相同硬件条件下

复现其他方法并与本课题方法进行对比实验测试，实验测试结果如表 3-2所示， 具体实验 PR曲线如图 3-8所示。 

表 3-2 不同方法实验测试对比结果 



|算法模型 |Precision |Recall |mAP |FPS |
| - | - | - | - | - |
|SSD |62\.56% |68\.33% |65\.23% |32\.01 |
|原始 YOLOv5 YOLOv5+SElayer |89\.49% 87.28% |76\.70% 71.91% |83\.20% 80.02% |**41.49** 32.78 |
|YOLOv5+CBAM |89\.18% |75\.59% |82\.34% |34\.84 |
|本课题方法 |**91.49%** |**82.03%** |**87.56%** |28\.09 |

可以看出当训练模型趋于稳定后：在查准率方面，本课题算法与 SSD方法 相比平均提升 28.93%，与原始 YOLOv5相比平均提升 2.00%，与 SElayer方法 相比平均提升 4.21%，与 CBAM方法相比平均提升 2.31%，说明本课题方法在 海滩小目标游客检测上可以取得更低的误检率。在查全率方面，本课题算法与 SSD方法相比平均提升 13.7%，与原始 YOLOv5相比平均提升 5.33%，与 SElayer 方法相比平均提升 10.12%，与 CBAM方法相比平均提升 6.44%，说明本课题方 法在海滩小目标游客检测上可以取得更低的漏检率。在平均精度均值方面，本课 题算法与 SSD方法相比平均提升 22.33%，与原始 YOLOv5相比平均提升 4.36%， 与 SElayer方法相比平均提升 7.54%，与 CBAM方法相比平均提升 5.22%，说明 本课题方法整体网络模型检测性能优秀。尽管本课题方法的检测速度略差，但考 虑到海滩游客的检测目的是保障游客的生命安全，在满足实时监测的前提下通过 牺牲检测速度提升检测率是有必要的。选取三幅海滩场景作为测试图片，通过视 觉效果进行主观评价，不同网络检测效果结果图如图 3-9所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.091.jpeg)

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.092.png)

图 3-8 不同方法实验测试 PR曲线图 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.093.png)

- a）SSD       （b）原始 YOLOv5   （c）SElayer       （d）CBAM       （e）本课题方法   

图 3-9 不同网络检测结果图 

从实验结果图可以看出，场景一实验图像中危险礁石区域实际共有 7名游客 信息，本课题方法可以很好地检测到危险礁石区域的 5名游客目标，CBAM仅 能检测到 4名游客目标，SElayer与原始 YOLOv5方法则没有很好地检测到该区 域的游客信息，仅检测到水中的 3名游客目标而忽略了礁石上的 2名游客目标， SSD方法没有任何一名游客目标信息。因为原始 YOLOv5本身并没有注重学习 小目标的特征，而 SElayer方法与 CBAM方法尽管有对小目标特征的学习但都 采取忽略图像中某些细节的方式减轻网络对目标关注的计算负担，由于画面中游 客目标信息量过少同时与周围环境差异性较小因此很难有效对其进行正确的预 测。而 SSD方法由于网络结构无法适应目标与背景信息相似的情况，无法很好 的提取出目标的特征，充分说明 YOLO方法实际效果优于 SSD方法，具有更强 的工程实用性。 

场景二实验图像中共有 11名游客目标小且与背景信息相似，除本课题方法 检测出 10名游客信息外，其他方法都出现大量漏检情况，例如原始 YOLOv5在 图像靠左与中间区域的游客信息都没有检出；SElayer方法仅对靠左区域的 3人 有效的检出，中间部分的 3名游客信息出现漏检；CBAM方法仅比原始 YOLOv5 在靠左区域有检测优势，右下角部分游客信息漏检严重，4名游客信息只检测出 3名；SSD方法在不仅在选取的部分没有检测到任何小目标游客而且整幅图只检 测到 1名游客信息。这是由于目标与背景信息相似型的目标在估计时产生许多交 叠的预测框使得在网络训练时没办法很好地对预测框的定位进行有效的收敛，进 一步证明本课题采用的损失函数可以更好地提升预测框的回归精度。 

场景三实验图像中水中的游客目标实际共计有 10名，相较于海滩游客特征 信息获取难度随着海水对身体的遮挡提高，本课题方法相比与其他主流方法取得 5名游客信息检出的最好效果。在图像中间的密集游客区域共有 11名游客信息， 同样本课题方法取得 8 名游客信息检测的最好结果，原始 YOLOv5 与 SElayer 方法都出现大面积的漏检情况，前者只检出3名游客信息而后者只检出4名。SSD 方法检测效率严重不足，选取的两个部分只有 1名游客信息，其他部分的游客信 息检测效果也不及本课题方法以及其他方法。本课题方法可以较大程度的检测出 绝大多数水中的小目标游客以及密集区域小目标游客，进一步说明了本课题方法 相对于其他方法可以更加细致地通过部分特征进行预测，同时由于采用了跨层信 息交流的 BIFPN结构，最后送给预测输出端的特征图包含了更多的上下文信息， 因此得到更好的检测率。与其他方法相比，本课题所提的一种多层特征图信息融 合的海滩小目标检测方法在保证实时性的前提下可以取得更精确的小目标游客 检测结果，在密集、遮挡、目标尺寸更小情况下都显示出更好的鲁棒性。 

5. Visdrone<a name="_page38_x87.00_y258.92"></a>数据集对比实验 

为更进一步证明本文所提改进的 YOLOv5海滩小目标游客检测算法的有效 性，在 Visdrone数据集[\[68\]](#_page70_x87.00_y308.92)上进行实验验证。Visdrone数据集是由天津大学等团队 开源的一个大型无人机视角的数据集，基准数据集包括 288 个视频片段，由 261908帧和 10209幅静态图像组成，该数据集以小目标、高密度、遮挡性、背 景复杂为特点。相同实验环境下对比本文算法与现有模型实验结果如表3-3所示。

表 3-3 Visdrone数据集对比实验结果 



|算法模型 |mAP |
| - | - |
|SSD |15\.24% |
|原始 YOLOv5 |31\.17% |
|YOLOv5+SElayer |30\.70% |
|YOLOv5+CBAM |33\.12% |
|本课题方法 |**34.34%** |

根据表中数据可以看出：本文算法相比于 SSD模型 mAP提升 19.10%，相 比于原始 YOLOv5算法 mAP提升 3.17%，与 SElayer方法比较 mAP提升 3.64%， 与 CBAM方法相比 mAP有着 1.22%的提升，从公开数据集的表现可以看出，本 文算法在复杂环境下的小目标检测任务中具有更好的检测性能。 

4. 本章小结<a name="_page38_x87.00_y634.92"></a> 

本章提出一种基于 YOLO的多层特征图信息融合（FMIF-YOLO）的检测方 法用于海滩环境下小目标游客检测，有效解决了当前智能监控检测方案无法满足 此类复杂场景下小目标游客的检测及其后续的准确操作，本课题方法可实时检测

海滩游客目标，有效保障危险区域中游客的人身安全。通过设计 GCSAM结构强 化主干网络端提取小目标特征的能力，所得到的特征图更加聚焦于小目标。将原 有的 FPN+PAN结构调整为 BIFPN结构进行跨层间的特征图信息交流，使得最 后送入预测输出端的特征图包含更多的上下文信息，提高在遮挡条件下的检出 率。通过幂变换的特性提升不同检测层预测框，使用更加精确的预测框回归损失 函数 Alpha-CIOU\_Loss，有效改善一些特定情况下回归预测框精度下降的问题。 最后通过实际的海滩环境进行改进检测网络的有效性实验，相比于原始 YOLOv5 模型，本课题方法在实时性满足要求的前提下查准率提升 2.00%，查全率提升 5.33%，平均精度均值提升 4.36%，且获得比其他主流方法更好的主观视觉评价。

<a name="_page40_x87.00_y78.92"></a>第四章 基于时空图卷积网络的溺水异常行为识别算

法研究 

1. 骨架关键点数据处理<a name="_page40_x87.00_y148.92"></a> 
1. 骨架关键点标定格式<a name="_page40_x87.00_y192.92"></a> 

参考 COCO数据集[\[69\]](#_page70_x87.00_y345.92)的人体姿态估计标定格式，本课题采用 18个部位关键 点的格式，18个关键点与人体结构对应关系如图 4-1所示。得到的人体骨架关键 点可以通过（X，Y，Score）的形式表示，其中（X，Y）表示某一个关键点在 二维平面所处的坐标位置，Score表示该关键点的置信度得分，得分越高说明该 点是人体部位的概率越大。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.094.jpeg)

图 4-1 人体骨架 18个关键点与对应部位示意图 

2. 基于<a name="_page40_x87.00_y517.92"></a> Alphapose的骨架关键点提取方法 

当前针对人体骨架关键点提取的方法主要分为两个大类，一类是以 Openpos[e\[70\]](#_page70_x87.00_y382.92)为代表的自下而上（Bottom-Up）方法以及另一类以 Alphapose[\[71\]](#_page70_x87.00_y436.92)为 代表的自上而下（Up-Bottom）方法。二者最大的不同在于提取骨架关键点的顺 序，Openpose 是采取先寻找图像中所有可能是骨架关键点的区域，再通过区域 之间的亲和关系将对应的关键点之间连线得到最后完整的人体骨架结构。 Alphapose则是先检测人体，再通过检测到的人体得到对应人体关键点并组合成 骨架信息。Openpose 凭借无需先检测人体的优势，在多人场景下检测速度与硬 件资源消耗相对于 Alphapose速度更快、消耗更少，但是 AlphaPose因为先检测 人体再估计骨架的流程，其准确率相比于 Openpose 有着巨大优势，同时经过 Alphapose处理的人体骨架可以很好的贴合被观测者所处的状态。考虑到本课题

针对海滩危险区域的游客溺水异常行为识别，出没在危险区域的游客通常不会数 量过多，而且一旦发生溺水事故，行为识别的准确率是第一要素，同时溺水情形 下游客仅存在上半身或者更少的肢体信息，此时使用 Openpose等自下而上的提 取方法会造成许多错误预测出的关节点，这可能对后续溺水行为判断产生不良影 响。综上所述，本课题采用基于 Alphapose的方法进行人体关键点的提取，得到 的关键点信息作为后续时空图卷积网络的输入。 

Alphapose网络整体框架如图 4-2所示。网络输入通过图像数据、视频数据 或者实时监控视频流数据的形式获得，接着二阶段检测方法 Faster-RCNN进行粗 略的单人人体检测，利用非极大值抑制筛去多余的预测框，将单人目标从原图裁 剪下来后缩放到一定比例大小作为下一步单人姿态估计[\[72\]（](#_page70_x87.00_y490.92)Single-Person  Pose Estimator，SPPE）的输入。单人姿态估计部分是基于堆叠沙漏模型而实现，但 仅使用该模型在实际单人姿态估计任务中会出现边界框定位错误与姿势冗余。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.095.jpeg)

图 4-2 Alphapose网络整体结构图 

Alphapose针对边界框定位错误的问题，提出一种附加在 SPPE上的对称空 间变换网络（Symmetric Spatial Transformer Network，SSTN），SSTN网络结构如 图 4-3所示，可以看出 SSTN由对称结构空间逆变换网络（Spatial De-Transformer Network，SDTN）与空间变换网络（Spatial Transformer Network，STN）构成， 利用 STN自适应输入图像的特点，进行平移、裁剪等 2D仿射空间变换操作将 经过目标检测得到的粗略目标处于检测框中央，以提高分类准确率，STN 表达 式为公式（4-1），其中（*xi* ,  *yis*）与（*xit*,  *yi*）表示映射前后的位置信息，*θi*表示

*s t*

空间变换操作矩阵元素。在 SPPE结束后需要将空间变换后的图像和估计出的人 体姿态骨架信息还原到输入图像中，此时通过 SDTN 逆仿射空间操作重新映射 回原始图像坐标，SDTN表达式为公式（4-2），其中 *Υi*表示逆变换操作矩阵元素。 Parallel  SPPE仅在训练过程中使用，作用是通过估计人体姿态来帮助 STN定位 到正确的中心位置，并获取高质量的人体区域位置，它相当于一种正则化操作。

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.096.jpeg)

图 4-3 SSTN网络结构图 

- *xt* ö
- *xs* ö ç *i* ÷
- *i* ÷ =[q q q ] ç *yt* ÷ 公式（4-1）
- *yis* ø 1 2 3 ç 1*i* ÷
  - ø
- *xt* ö =[ ] æç *xis* ÷

ö

- *i* ÷ g1 g 2 g3 ç *yis* ÷ 公式（4-2）
- *yit* ø ç 1 ÷
  - ø

Alphapose 针对姿态冗余的问题，提出了一种参数化姿态非极大值抑制

- Parametric Pose NMS），它以置信度最高的姿态为参考，根据一定的标准消除 一些与它相近的姿态。具有 m个关节的姿态 P*i*可以表示为{（k*i* , c*i*）, …,（k*i*m, 

1 1

c*i*m）}，其中 k*ij*和 c*ij*分别表示第 j个关节的位置与置信度分数。公式（4-3）为 置信度消除标准，计算出的姿态距离相似性 d（P*i*, P*j* | Λ）与人工设定好的阈值 *η* 进行比较判断是否需要消除该姿态。 

*f* (P ,P | L,h) = 1[d(P ,P | L,l) £ h] 公式（4-3） *i j i j*

最后将处理完成的图像进行显示并保存。 

3. Alphapose<a name="_page42_x87.00_y524.92"></a>算法优化 

从上一小节 Alphapose网络结构分析中可以看出，自上而下的姿态估计方法 十分依赖单人检测器所提供的人体边界框，如果人体边界框定位不准确就无法正 确预测人体姿态，因此人体边界框的质量很大程度决定后续能否顺利进行人体姿 态估计，而原始 Alphapose中采用二阶段目标检测方法 Faster-RCNN来进行单人 人体检测，尽管该方法有着较高的检测精度，但是考虑到应用在海滩场景保证游 客生命安全，因此对算法的检测速度有着较高的要求。 

对此，本课题引入 YOLOv5检测方法代替 Faster-RCNN进行单人人体检测， 一方面 YOLOv5 检测速度远远高于 Faster-RCNN，提高模型运行速度同时 YOLOv5有着媲美 Faster-RCNN的检测精度，另一方面充分复用了第三章中所提 YOLO模型，通过更换预测所使用的权重文件进行大中目标的半身人体检测，避 免引入过多的算法模型，保证系统正常运行的同时减少了整个系统的大小。 

2. 融合改进<a name="_page43_x87.00_y118.92"></a> Alphapose的时空图卷积网络 
1. 基于时空图卷积网络的人体行为识别方法<a name="_page43_x87.00_y186.92"></a> 

人体骨架结构信息在行为识别过程中相比于外形、光流等方式具有更好的鲁 棒性，对人体行为识别准确率的提升主要利用人体骨架信息可以很好的约束人体 后续行为动作。骨架信息由关节点与肢体连线段组成，是一种十分理想的图结构， 因此可以利用图卷积神经网络（GCN）对单帧人体姿态估计信息进行空间上的 骨架点信息特征提取，考虑到行为动作是一个连续时间段所构成，所以时间信息 同样需要考虑进入整个网络模型之中。 

时空图卷积神经网络[\[73\]](#_page70_x87.00_y527.92)（Spatial  Temporal  Graph  Convolutional  Network， ST-GCN）可以自动捕捉嵌套在关节点空间结构以及时序动态中的模式，不仅避 免了人工设计计算准则，同时强化了网络的泛化表达能力，而且对时间信息进行 了分析，充分利用了不同关节之间的空间关系以及相同关节之间的时间关系，使 得神经网络对人体骨架动作此类非欧式空间数据能够进行充分理解后正确表达 其含义。ST-GCN进行行为识别整体流程如图 4-4所示，输入数据通常为一系列 视频帧，通过姿态估计的方式每一帧图像输出一组对应的关节坐标，将关节点坐 标作为图的顶点（Vertices），以单帧内的人体关节自然肢体连接构建空间图的连 边（Edges）,以相同节点的连续时间帧的自然过渡构建时间图的连边，利用多次 时空图卷积操作，在原始图结构上提取更深层次的特征，生成信息更丰富的特征 图。最后对应的动作类别经过标准的分类器分类得到。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.097.jpeg)

图 4-4 ST-GCN整体流程图 

在骨架的图结构构建上，G=（V，E）表示一个由 T帧骨骼序列和 N个节点 所组成的时空图，其节点的集合为 V={v*ti* | t=1，…，T，i = 1，...，N}，第 t帧中 的第 i个节点的特征向量 **F**（v*ti*）包含该节点的置信度分数和坐标向量。图结构 由两部分所组成：（1）根据人体结构，将每一帧的关节按一定顺序连接构成空间 连边 Es={v*ti*，v*tj* | t=τ，（*i*，*j*）∈H}，H表示一组自然连接的人体关节；（2）将两 帧连续图像中相同部位的节点组成时间连边 Ef ={ v*ti*，v（*t*+1）*i* }，骨架的图结构示 意图如图 4-5所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.098.png)

图 4-5 骨架的图结构示意图 

在时空图卷积方面，利用图卷积神经网络的规则，首先对空间上所有节点进 行区域划分，ST-GCN采用空间构型划分，有别于基于距离划分（节点的 1邻域 划分为两个部分，包括节点本身部分与邻节点部分）与唯一划分（节点的 1邻域 作为一个单独部分）。空间构型划分如图 4-6（d）所示，空间构型划分将节点的 1邻域划分为三个部分，第一个部分为根节点本身，第二个部分连接了比根节点 更远离整个骨架的空间位置上的邻居节点，第三个部分连接了更接近中心的邻居 节点，分别代表了静止的运动、离心运动和向心运动特征。利用图卷积神经网络 的规则，ST-GCN首先对空间上所有节点进行区域划分，以解决时空图卷积中的 姿态冗余问题，然后通过公式（4-4）进行空间上的图卷积操作，其中 *Zti*（*vtj*） 为正则化项，用于平衡不同类别的动作，p（**·**）表示采样函数，w（**·**）为对应节 点之间的权重函数。最后将时空信息考虑进空间图中得到最终的时空图模型，将 空间图中的邻域 *B*（*vti*）进行扩充，使其包含时间上的连续节点，即将 *B*（*vti*）

改写为公式（4-5），其中 Γ用于控制要包含在相邻图中的时间范围。 

*f* = å 1~~ *f* (P(*v* ,*v* ))￿w(*v* ,*v* ) 公式（4-4）

*out Z* (*v* ) *in ti tj ti tj*

*vtj*Î*B*(*vti*) *ti tj*

*B*(*v* )={*v* |d(*v* ,*v* )£ *K*, *q*−*t* £êëΓ/ 2úû} 公式（4-5）

*ti qj tj ti![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.099.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.100.png)*

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.101.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.102.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.103.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.104.png)

- a）骨架结构图          （b）唯一划分          （c）基于距离划分   （d）空间构型划分    

图 4-6 空间构型划分示意图 

最终，尽管时间上关节变化的局部特征仍然缺失，但 GCN能够学习到空间 上邻近关节之间的局部特征。由于人体关节点数目是固定的且输入图像大小相 同，因此可以采用时间卷积（Temporal Convolutional Network，TCN）来学习时 间上关节变化的局部特征，类似于图像的 2D卷积操作，ST-GCN的特征图最后 三个维度形状为（C，V，T），与图像特征图形状（C，W，H）相类似。在时间 卷积中，卷积核大小为 t×1，则每次完成 1个节点、t个关键帧的卷积，1个节 点接着 1个节点完成卷积操作，类比在图像卷积中，卷积核大小为 w×1，则每 次完成 1列像素、w行像素的卷积，1行接着 1行完成卷积操作，完成 1个节点 后进行下 1个节点的卷积，ST-GCN的时间卷积操作示意图如图 4-7所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.105.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.106.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.107.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.108.png)

图 4-7 时间卷积操作示意图 

2. 融合改进<a name="_page45_x87.00_y371.92"></a> Alphapose的时空图卷积网络模型结构 

原始 ST-GCN通过预设的 HRNe[t\[74\]](#_page70_x87.00_y581.92)等自下而上的方法进行单人人体姿态估 计，考虑到本课题应用在海滩危险区域的游客异常行为检测，其中主要是溺水行 为检测。当发生溺水行为时，可以被观测到的人体只有上半身甚至更少，HRNet 等自下而上的方法需要先进行关节点的估计再得到完整的人体骨架信息，此时游 客的下半身几乎是不可见的，若强行进行估计所得到的骨架信息必然存在着极大 的误差。针对该问题，本课题采用改进检测器的 Alphapose方法作为 ST-GCN网 络骨架信息的提供网络，改进后的 Alphapose由 YOLOv5提供单人姿态估计，可 以获得更加精确的单人人体预测框以便关节点提取网络进行骨架信息提取。同 时，在运动过程中不同的关节重要程度是不同的，例如在溺水动作中，手部、头 部的动作可能比脚部等部位更加重要，通过手部与头部的位移程度可以很好区分 岸上正常行走与溺水行为。因此，在 ST-GCN中引入注意力机制对身体的不同区 域进行适当的加权，对重要的部位进行更加细致的特征提取。融合改进 Alphapose 的时空图卷积网络模型结构如图 4-8所示。 

当构造好的骨架图结构进入 ST-GCN 网络后，首先通过一个 BN（Batch Normalization）层对数据进行归一化操作，防止不同类型的动作之间存在过分的 差异，之后经过由注意力机制模块（ATT）、图卷积神经网络（GCN）和时间卷 积神经网络（TCN）[\[75\]](#_page70_x87.00_y635.92)所组成的 ST-GCN结构，每一个 ST-GCN采用残差网络 的结构，前三层的输出有 64个通道，中间三层有 128个通道，最后三层有 256 个通道，最后进入池化层与全连接层将特征信息转成一维向量后通过分类器进行 动作类型的输出。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.109.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.110.png)

时空图卷积网络![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.111.png)

输入视频 单人姿态 输入骨架 构造骨架 输出动作 人体检测 **ST-GCN**

序列 估计 序列 图结构 分类

改进**AlphaPose**

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.112.png)

图 4-8 融合改进 Alphapose的时空图卷积网络模型结构图 

3. 实验及结果分析<a name="_page46_x87.00_y314.92"></a> 
1. 数据集的建立<a name="_page46_x87.00_y370.92"></a> 

考虑到当前针对溺水行为识别的视频数据集都是在游泳馆等场所，利用水下 摄像头所拍摄的人体水下姿态信息，但本课题所决解的场景为露天海滩场所，该 场景下无法获得游客水下姿态信息，因此本课题选择采用自建人体水上溺水行为 视频数据集来验证本课题所提方法的有效性。数据集通过 Python脚本对互联网 视频网站进行关键字抓取，将带有“溺水”关键字的视频下载到本地后，通过人 工筛选出符合条件的上半身溺水视频集合，利用剪辑软件对视频进行片段裁剪， 选取其中涉及到溺水行为的片段，将视频帧率随机设成 25fps - 30fps，视频时长 根据溺水行为实际发生时间而定，本课题所用数据时长一般为 3s - 10s不等。最 终得到共计 378段溺水行为视频数据集，同时通过 UCF101[\[76\]](#_page70_x87.00_y672.92)引入 378段非溺水 行为视频数据集与本课题自制数据集形成最终数据集，通过网络训练得到可以区 分溺水行为的行为识别网络模型，部分溺水行为视频数据截取图如图 4-9所示。

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.113.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.114.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.115.png)

- a）部分溺水行为实验图一    （b）部分溺水行为实验图二     （c）部分溺水行为实验图三     

图 4-9 部分溺水行为视频数据截取图 

2. 实验环境配置与评价指标<a name="_page47_x87.00_y78.92"></a> 

本课题实验配置环境如下：Ubantu18.04操作系统、CPU为Intel Core i9-8950、 GPU为 NVIDIA GeForce GTX 1080（8GB显存），内存 32GB。基于 Pytorch架 构使用 Python3.8环境和 GPU加速平台 CUDA  10.1实现本课题方法。ST-GCN 训练过程中采用 Dropout为 0.50的神经元随机失活以提高网络泛化能力，初始学 习率设为 0.01，每 10 次迭代学习率下降 10%，采用随机梯度下降优化方式， Momentum动量设为 0.90。 

本课题采用如下实验指标对部分实验进行客观评价：Top1 准确率（Top1 Accuracy），该指标表示预测概率排名第一的类别与实际结果相符的准确率，本 课题将分类器所得到最大概率的某一个类别的作为最后的识别结果，如果得到的 类别与实际的类别相同则表示预测成功。本课题将溺水行为归为正样本（Positive Sample），将其他非溺水行为归为负样本（Negative  Sample），Top1 精度表达式 为公式（4-6），其中 *xi*的实际类别为 *yi*，（**·**）为预测类别函数，总类别数为 *N*， 本课题中 N=2。 

å*N*−1( *f* (*x* )== *y* )

*i i* 公式（4-6） Top1 = *i*=0

acc*uracy N*

3. 姿态估计方法对比<a name="_page47_x87.00_y436.92"></a> 

为证明本课题中自上而下的姿态估计方法相比于自下而上的姿态估计方法 有着更好的容错率，本课题采用自上而下的姿态估计方法 HRNet与自下而上的 姿态估计方法 Alphapose进行主观视觉效果评价。部分实验图经过不同模型得到 的人体姿态估计结果如图 4-10所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.116.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.117.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.118.png)

- a）HRNet实验图一          （b）HRNet实验图二          （c）HRNet实验图三         

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.119.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.120.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.121.png)

- d）Alphapose实验图一        （e）Alphapose实验图二        （f）Alphapose实验图三     

图 4-10 HRNet与 Alphapose对比实验结果图 

从实验结果图可以看出：实验图一中，人体以一个较为完整的形式出现，此 时不论是自下而上的方法 HRNet还是自上而下的方法 Alphapose都可以很好的 对人体不同区域的姿态进行预测，并组成完整的人体骨架信息，充分说明了不同 原理的姿态估计方法在完整人体情况下对关节点预测的有效性，二者几乎没有什 么本质上的区别。 

实验图二中，人体已经处于溺水行为状态，可以发现人体只有头部、手部以 及部分肩部能够被清晰观测。当采用 HRNet方法对该目标进行骨架信息预测时， 可以被观测到的区域依旧很好地进行了对应的标志，但人体下半身无法被观测到 的区域也同样被 HRNet方法标注出来，若此时对关节点进行肢体连线，所得人 体姿态显然与实际情况不相符，当网络对此骨架信息进行识别，得到的动作可能 并不是溺水行为。当采用 Alphapose方法对该目标进行骨架信息预测时，可以发 现 Alphapose仅对可观测到的部分进行了有效的预测显示，对于不可见的部分则 不做判断，尽管这样得到的骨架信息相比于 HRNet方法少了许多，但 Alphapose 并没有对不可见的关键点进行猜测，利用注意力机制可以很好地关注有限的关节 点对整个行为的识别判断，当一些信息无法被判断是否正确时，最好的方法就是 利用好已知正确的信息，防止不确定的因素对网络判断造成不可预测的影响。 

实验图三中，人体同样处于溺水行为状态，同时可以被观测到的部分更少。 此时 HRNet方法不仅在单人检测框内给出了不可见的关节预测点，而且在检测 框外同样存在不可见的关节预测点，当给出的预测点出现在检测框外时，基本可 以判断该姿态估计方法所提供的关节点是错误的，使用错误的关节点必然无法得 到正确的行为类别，同时也会很大程度影响网络的训练过程。反观 Alphapose方 法，依旧只针对可见部分进行关节点的预测估计，同时 YOLOv5所得到的预测 框很好地包含了整个可见人体，进一步说明了在海滩游客危险区域溺水行为识别 过程中，Alphapose方法所提供的关节点信息虽然较少但是正确，HRNet方法所 提供的关节点信息虽然数量多但错误率较高，在溺水行为识别中采用 Alphapose 所提供的关节点训练 ST-GCN网络可以得到更好的分类结果。 

4. 算法性能测试及对比实验<a name="_page48_x87.00_y598.92"></a> 

为进一步验证本课题所提融合改进Alphapose的时空图卷积网络模型的有效 性，本课题采用原始的 ST-GCN（HRNet + ST-GCN）、以 Faster-RCNN为检测器 的 Alphapose + ST-GCN以及本课题所提方法（以 YOLOv5为检测器的 Alphapose +  ST-GCN）进行客观指标比较，在相同硬件条件下使用本课题自制溺水行为数 据视频集统一训练网络，并对本课题网络效果进行验证。原始 ST-GCN的 Top1 准确率为 35.93%，以 Faster-RCNN为检测器的 Alphapose + ST-GCN的 Top1准

确率为 65.45%，本课题所提方法的 Top1准确率为 71.59%，本课题所提算法相 比于原始 ST-GCN方法 Top1准确率提升 35.66%，以 Faster-RCNN为检测器的 Alphapose + ST-GCN相比于原始 ST-GCN方法 Top1准确率提升 29.52%，充分说 明在海滩溺水行为识别过程中，自下而上的姿态估计方法在目标姿态不可见的情 况下会强行进行骨架点的估计预测，此时错误的骨架点会导致网络进行错误的判 断，而自上而下的方法利用好有限的骨架点信息对网络进行充分的训练，可以很 好的区分出溺水行为与非溺水行为之间存在的差异性，得到正确的行为判断。本 课题所提算法相比于以 Faster-RCNN为检测器的 Alphapose + ST-GCN方法 Top1 准确率提升 6.14%，有效说明了采用更加优秀的单人人体检测器可以得到更好的 骨架结构，而更好的骨架信息可以更加有效地帮助网络对不同类别的行为进行区 分判断，得到正确的分类结果。 

本课题方法实际检测效果如图 4-11所示。从测试结果图中可以看出：通过 Alphapose得到的骨架点信息送入 ST-GCN网络进行特征抽取，利用注意力机制 关注动作剧烈变化的关节点，并对动作进行分类，对注意力关节点的可视化，可 以很好的帮助本课题分析网络的预测过程中所重点关注的肢体信息。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.122.jpeg) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.123.png)

- a）溺水行为识别                    （b）非溺水行为识别                   

图 4-11 本课题所提方法实际检测效果图 

4. 本章小结<a name="_page49_x87.00_y625.92"></a> 

本章提出一种融合改进Alphapose的时空图卷积网络模型用于海滩场景下危 险区域游客异常行为识别（主要是溺水行为识别），有效解决了当前针对溺水行 为中能被检测到的骨架点信息缺失导致行为判断准确率较低的问题。首先，通过 Alphapose等自上而下的方法提取人体骨架信息排除环境对行为识别的干扰，引 入 YOLOv5作为单人人体检测器，相比于原网络中采用 Faster-RCNN检测器， 得到更快更好的人体检测框；其次，将 Alphapose 代替自下而上的方法 HRNet 所得的人体关节信息送入 ST-GCN网络训练，基于人体检测框所得骨架信息点有 效避免了水下人体不可观测区域被错误估计关节点送入 ST-GCN网络的情况，提 高行为识别的精确度；接着，通过自制海滩溺水行为数据视频集弥补当前针对海 面溺水行为数据不足的问题；最后，进行实验验证得到本课题所提方法相比于原 始 ST-GCN方法 Top1精确度提升 35.66%，相比于以 Faster-RCNN为检测器的 Alphapose + ST-GCN方法 Top1精确度提升 6.14%，在复杂的海面场景下可以有 一个较好的溺水异常行为识别判断。

<a name="_page51_x87.00_y78.92"></a>第五章 海滩监控的小目标检测和行为识别系统研究 

1. 系统需求与实现环境<a name="_page51_x87.00_y128.92"></a> 
1. 应用背景与功能需求<a name="_page51_x87.00_y172.92"></a> 

金井镇围头村位于晋江东南面，东临台湾海峡，距离大金门岛仅有 5.2海里。 围头湾海水水质优越，每年都会吸引众多水上爱好者慕名前来，旅游业已然成为 推动当地经济发展的特色产业。游客在选择景点时往往将金沙湾海滩作为游玩的 必经之地，正因为金沙湾每年都会有许多游客进入景区内的海滩游玩，所以每年 都会有游客溺水事件的发生。 

根据调研情况（实际调研场景如图 5-1所示），本课题发现围头村 2020年部 署投入使用的“村村通”监控系统，通过目前在海滩上的云台摄像头以及每天人 工坚守值班室，从而起到监控的作用。相较于部署系统之前，节约了至少六个每 天实地监控海滩礁石区的巡航人员。尽管如此，值班室仍然需要 24小时有人员 值班，并通过手动控制鼠标的方式对画面进行操作，达到扩大监测范围的目的。 在浴场礁石区也设有黄线提醒游客前方危险，但仍有游客忽视黄线警告不断向礁 石区边缘靠近，此时值班室通过人工喊话的方式对游客进行劝返，系统虽有配备 自动劝返功能但效果甚微，因为目前系统对画面中出现会产生位移的事物便会触 发警告，容易对海浪、潮水以及处于安全区的游客误判，导致不断重复没有针对 性的警告，游客较多时无法让危险游客意识到自己的安危，无法起到劝返作用。

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.124.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.125.png)

图 5-1 实际调研场景图 

针对当前围头村海滩监控系统依靠人力、危险区域游客劝返途中人生安全无 法得到保障的问题，本课题通过结合第三章与第四章所提方法，对海滩监控系统 进行智能化改造，设计出一款自动检测危险区域小目标游客信息并在劝返途中关 注游客异常行为发生的图像界面 GUI（Graphical User Interface）可视化软件，使 得系统能够 24小时自主监控危险区域游客出没情况，当危险区域出现目标游客 后，系统及时提醒安全员或者出动无人机设备对目标进行劝返，同时在劝返途中 提防游客发生溺水等异常行为，当出现溺水等异常行为时，及时进行更进一步的 响应操作。 

2. 开发环境与开发工具<a name="_page52_x87.00_y118.92"></a> 

本课题在开发 PC系统为 Ubantu18.04操作系统、CPU为 Intel Core i9-8950、 GPU为 NVIDIA GeForce GTX 1080（8GB显存），内存 32GB为基础的硬件条件 下进行海滩小目标检测与行为识别监控系统开发。 

本课题采用Pycharm Professional作为Python的集成开发环境使用Python编 程语言作为主要的系统开发编程语言，Pycharm是 JetBrains开发的一款 Python IDE（集成开发环境），它为用户提供了一系列工具，可以帮助他们用 Python语 言进行高效的开发。它有图形用户界面，可以在 IDE 中完成所有开发任务，同 时具有代码补全与检查的功能，并快速修复问题，还有强大的调试器、解释器与 编译器，支持设置断点与单步执行等功能。 

本课题采用基于 Pytorch的框架对所用到的小目标检测模型与行为识别模型 进行搭建与调试，Pytorch 作为当前主流的深度学习神经网络框架，其最大的两 点就是实现强大的 GPU加速以及包含自动求导系统的深度神经网络，这是许多 主流框架所不支持的，同时 Pytorch还有写代码快、强大的社区支持、代码简单、 移植方便等优点，本课题在该框架下进行开发便于后期维护与改进。 

本课题采用 Qt Designer完成整个系统的 GUI可视化，Qt Designer是 Qt SDK 的重要开发工具之一，也是一个可视化的全功能 GUI构建器，它设计出来的用 户界面可以在多个平台上运行。本课题可以利用 Qt Designer，拖放各种 Qt控件 来构建图形用户界面，并预览效果，利用 pyuic快速转换成“.py”程序，进而快 速完成界面开发，设计起来也更加方便清晰。 

2. 系统总体设计<a name="_page52_x87.00_y548.92"></a> 
1. 系统硬件框架<a name="_page52_x87.00_y616.92"></a> 

系统硬件框架示意图如图 5-2所示，本课题主要硬件框架由两部分组成，即 前端感知与后端平台。前端感知主要以部署在海滩周边的高清云台为主，依赖这 些高清云台得到海滩实况画面。前端感知与后端平台之间依靠本地局域网实现高 清视频流信号的高效传输。后端平台由存储设备、智能分析服务器以及显示设备 组成，前端高清云台所获得的视频流数据送入已经完成训练部署的智能分析服务

器进行相应的小目标检测以及后续的溺水异常行为识别分析，同时将视频流数据 备份在本地存储设备中方便后期调用查看，检测结果可以通过 GUII可视化界面 在显示设备上进行查看。 

海滩监控的小目标检测和行为识别系统![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.126.png)

存储设备      智能分析服务器        显示设备![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.127.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.128.png)![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.129.png)

本地局域网

高清云台

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.130.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.131.png)

图 5-2 系统硬件框架总体示意图 

2. 系统软件框架<a name="_page53_x87.00_y411.92"></a> 

系统软件框架示意图如图 5-3所示，整个系统软件框架主要由四部分构成， 分别是数据采集层、数据处理层、数据存储层以及结果显示层。数据采集层中， 主要依靠高清云台实时传回的视频流数据采集数据。数据处理层中，依靠 Python 编程语言所对应的 Pytorch、Opencv与 CUDA等深度学习基本框架结构，完成小 目标检测模型、溺水等异常行为识别模型的训练与部署，当所采集的数据满足一 定条件时，启动对应的功能对场景进行分析判断。数据存储层中，将原始数据和 经过算法模型处理过的数据利用对应的 API接口统一保存在本地存储设备之中， 以便后期查阅。结果显示层中，通过 Qt Designer所设计的 GUI交互界面展示对 应的模型结果，界面同时还存在其他功能按键，支持多种操作。 



|结果显 示层|数据处理结果展示|
| :-: | - |

**Qt Designer![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.132.png)**



|数据保 存层|视频存储 **API**接口|
| :-: | - |



|数据处 理层|小目标检测 异常行为识别|
| :-: | - |

**Pytorch   Opencv CUDA...![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.133.png)**



|数据采 集层|高清云台|
| :-: | - |

图 5-3 系统软件框架总体示意图 

3. 系统整体流程<a name="_page54_x87.00_y78.92"></a> 

系统整体流程示意图如图 5-4所示。首先，本系统将高清云台部署在海滩需 要检测的危险区域如礁石区等场景，将云台通过本地局域网与后端服务器进行连 接，实时采集视频流数据并回传给服务器与存储设备；之后，智能分析服务器通 过提前训练部署完成的小目标游客检测模型针对预先设定好的需要被观测的危 险区域开展游客目标检测，若此时区域中出现游客目标，一方面系统及时通过喇 叭等传声设备向游客进行劝返操作并告知现场值班人员，同时驱动云台对该危险 区域进行进一步的画面放大便于下一步溺水等异常行为识别，否则继续检测危险 区域是否出现游客目标；最后，继续通过云台回传实时画面，当收集到的视频流 达到一定规模时服务器实时分析游客是否发生溺水等异常行为，此时若游客出现 溺水等异常行为，则立刻告知现场值班救援人员，并启动相应的救援响应，否则 系统持续劝返并关注游客行为直至游客退出危险区域。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.134.png)

图 5-4 系统整体流程总体示意图 

3. 系统功能模块与可视化界面搭建<a name="_page54_x87.00_y469.92"></a> 
1. 系统主界面<a name="_page54_x87.00_y525.92"></a> 

系统主界面是用户所见到第一个界面，其承担着告知用户本系统的整体功能 以及整个系统由几个部分所构成及各部分功能的任务。本课题系统主界面应包括 如下内容：（1）系统名称——海滩小目标检测与溺水行为识别系统，开发人员、 开发单位和开发年份；（2）退出系统按键；（3）各部分功能跳转按键，包括：不 进行任何小目标与行为识别的普通监控界面跳转按键、海滩小目标游客危险区域 检测界面跳转按键以及危险区域游客溺水行为异常识别界面跳转按键。通过 Qt Designer工具进行设计：利用 Layouts界面布局函数调整界面整体水平、垂直布 局设计并完成后续控件摆放位置，使用 Qlabel 控件设置系统名称、开发人员、 开发单位和开发年份，最后拖入 QPushButton控件设置各个子模块界面跳转按键 与系统退出按键。系统主界面开发过程如图 5-5所示，表 5-1给出搭建主界面时 候所用到的所有控件信息。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.135.jpeg)

图 5-5 系统主界面开发过程图 表 5-1 主界面各控件信息 



|控件序号 |控件类别 |控件名称 |控件功能 |
| - | - | - | - |
|1 |Qlabel |label |显示系统名称 |
|2 |Qlabel |label\_1 |显示开发信息 |
|3 |QPushButton |pushbutton |系统退出按键 |
|4 |QPushButton |pushbutton\_1 |普通监控跳转按键 |
|5 |QPushButton |pushbutton\_2 |危险区域检测跳转按键 |
|6 |QPushButton |pushbutton\_3 |溺水行为识别跳转按键 |

2. 接入监控视频界面<a name="_page55_x87.00_y556.92"></a> 

本系统提供普通监控界面，此时通过选择需要调用的摄像头信息，将实况画 面在界面上进行显示，通过按键选择是否启动海滩危险区域游客小目标检测模 型。若选择启动，则会跳转到海滩小目标游客检测界面进行相应的操作，同时还 提供跳转主界面按键，加强不同界面之间的联动性。通过 Qt  Designer工具进行 设计：利用 Layouts界面布局函数调整界面整体水平、垂直布局设计并完成后续 控件摆放位置，使用 QFileDialog控件选择系统目录找到所需要的监控摄像头或 视频片段，QTextBrowser控件用于显示在 QFileDialog中所得到的路径信息，经 过 QVideoWidget控件在界面上显示读取到的视频信息，QPushButton控件设置 一些对应的操作与跳转界面。接入监控视频界面开发过程如图 5-6所示，表 5-2 给出接入监控视频界面所用到的控件信息。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.136.jpeg)

图 5-6 接入监控视频界面开发过程图 表 5-2 接入监控视频界面各控件信息 



|控件序号 |控件类别 |控件名称 |控件功能 |
| - | - | - | - |
|1 |QFileDialog |filedialog |选择所需摄像头或视频 |
|2 |QTextBrowser |textbrowser |显示被选择的目标目录 |
|3 |QVideoWidget |videowidget |播放视频画面 |
|4 |QPushButton |pushbutton\_1 |确定目录信息并播放 |
|5 |QPushButton |pushbutton\_2 |清空目录并重新选择 |
|6 |QPushButton |pushbutton\_3 |危险区域检测跳转按键 |
|7 |QPushButton |pushbutton\_4 |返回主界面跳转按键 |

3. 海滩小目标检测界面<a name="_page56_x87.00_y605.92"></a> 

海滩危险区域小目标游客检测界面首先需要通过在界面画面中绘制待检测 的危险区域，一方面记录危险区域画面的坐标信息，另一方面限定模型检测画面 范围，只针对危险区域做游客目标检测而非全局画面可以有效节约运算资源。运 行第三章训练部署好的 FMIF-YOLO模型，若此时所圈定的危险区域中出现游客 目标，给出相应的提示，并自动跳转进入溺水行为异常识别界面进行进一步的跟 踪。同样通过跳转主界面按键，加强不同界面之间的联动性。通过 Qt  Designer 工具进行设计：利用 Layouts界面布局函数调整界面整体水平、垂直布局设计并 完成后续控件摆放位置，经过 QVideoWidget控件在界面上显示模型运行结果， Qlabel控件设置对应提醒标语，QPushButton控件设置绘制待检测区域操作与接 入视频操作与跳转界面。海滩小目标检测界面开发过程如图 5-7所示，表 5-3给 出海滩小目标检测界面所用到的控件信息。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.137.jpeg)

图 5-7 海滩小目标检测界面开发过程图 表 5-3 海滩小目标检测界面各控件信息 



|控件序号 |控件类别 |控件名称 |控件功能 |
| - | - | - | - |
|1 |QVideoWidget |videowidget |显示检测结果画面 |
|2 |Qlabel |label |出现小目标给出提示 |
|3 |QPushButton |pushbutton\_1 |得到接入视频流信息 |
|4 |QPushButton |pushbutton\_2 |绘制待检测危险区域 |
|5 |QPushButton |pushbutton\_3 |返回主界面跳转按键 |

4. 溺水行为异常识别界面<a name="_page57_x87.00_y655.92"></a> 

在海滩小目标检测界面检测到危险区域游客出没后，通过绘制危险区域过程 中所记录的坐标信息，控制云台进行偏转并放大危险区域画面得到更加精确的危 险区域画面与游客人体信息。海滩小目标检测界面跳转进入溺水行为异常识别界 面，此界面通过云台放大后的画面信息结合第四章所提融合改进 Alphapose的时 空图卷积网络模型，对放大后的目标游客进行骨架点的提取并根据骨架运动信息 判断当前游客是否处于溺水等异常行为状态，若此时出现溺水状态，扬声器给出

警报。该界面还提供算法模型测试按键与跳转主界面按键。通过 Qt  Designer工 具进行设计：利用 Layouts界面布局函数调整界面整体水平、垂直布局设计并完 成后续控件摆放位置，经过 QVideoWidget 控件在界面上显示模型运行结果， QPushButton控件提供算法模型测试按键与跳转界面。溺水行为异常识别界面开 发过程如图 5-8所示，表 5-4给出溺水行为异常识别界面所用到的控件信息。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.138.jpeg)

图 5-8 溺水行为异常识别界面开发过程图 表 5-4 溺水行为异常识别界面各控件信息 



|控件序号 |控件类别 |控件名称 |控件功能 |
| - | - | - | - |
|1 2 3 |QVideoWidget QPushButton QPushButton |videowidget pushbutton\_1 pushbutton\_2 |显示行为识别结果画面 行为识别算法模型测试 返回主界面跳转按键 |

4. 系统功能测试与实现效果<a name="_page58_x87.00_y637.92"></a> 

上一节主要通过分析每一个界面的需求介绍了其布局思路，本节将上一节所 述 GUI界面结合第三章和第四章所提算法模型进行对应的功能性测试。 

1. 选择视频流与小目标检测功能测试<a name="_page59_x87.00_y78.92"></a> 

接入监控视频界面选择视频流功能测试结果如图 5-9（a）所示。通过选择目 录下对应的视频数据，可以在界面上观察到所选视频数据目录信息，点击确定， 视频流数据在接入监控视频界面预设位置显示画面。通过接入监控视频界面跳转 进入海滩小目标检测界面，使用绘制危险区域按键在显示区域绘制所需要监控的 海滩危险区域，按下接入视频流按键，从上一个界面获得视频信息，模型开始启 动进行检测，检测结果显示在海滩小目标检测界面显示区域，小目标检测功能测

试结果如图 5-9（b）所示。图 5-10为 FMIF-YOLO模型运行展示，图 5-11为只 针对危险区域进行游客目标检测与对全局进行游客目标检测对比结果。从实验结

果图可以看出，小目标检测测试功能正常，模型可以在指定的危险区域精确运行 并检测出小目标游客出没情况，并给出提醒进行页面跳转。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.139.jpeg) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.140.jpeg)

- a）接入监控视频界面功能测试结果                （b）约束区域小目标检测功能测试结果 

图 5-9 接入监控视频界面与约束区域小目标检测功能测试结果图 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.141.png)

图 5-10 在系统中运行 FMIF-YOLO模型示例图 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.142.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.143.png)

- a）对危险区域进行游客目标检测               （b）对全局进行游客目标检测              

图 5-11 对危险区域进行游客目标检测与对全局进行游客目标检测对比图 

2. 溺水行为识别功能测试<a name="_page60_x87.00_y224.92"></a> 

当海滩小目标检测界面指定危险区域出现游客目标，给出“待检测区域出现 游客，即将跳转进入行为识别界面…”提示，系统自动跳转进入溺水行为异常识 别界面并启动行为识别模型对云台放大后的画面进行游客行为识别，在界面上展 示识别结果，溺水行为识别功能测试结果如图 5-12所示，点击“行为识别算法 模型测试”按键，测试系统对溺水与非溺水行为识别结果，如图 5-13所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.144.jpeg)

图 5-12 溺水行为识别功能测试结果图** 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.145.png) ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.146.png)

- a）非溺水行为                          （b）溺水行为                        

图 5-13 非溺水与溺水行为模型运行结果图 

3. 系统实际部署运行状态<a name="_page61_x87.00_y78.92"></a> 

最后，展示实物上运行本课题所提海滩监控的小目标检测和行为识别系统。 运行进入系统主界面，主界面显示内容至上而下包括：系统应用名称——海滩小 目标检测与溺水行为识别系统、选择输入视频信息界面跳转按钮、海滩小目标检 测界面跳转按钮、溺水行为识别界面跳转按钮、退出系统按钮、开发者姓名缩写 与单位名称以及开发年份，具体画面展示如图 5-14所示。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.147.jpeg)

图 5-14 系统主界面运行展示图 

系统各功能实物展示效果示意图如图 5-15所示。图 5-15（a）为接入监控视 频界面运行展示，由主界面选择“选择需要输入的视频流”按钮进入，当选择好 所需调用的视频流后点击“确定”按钮，系统调用画面并在界面上进行实时显示， 此时可以在界面选择跳转进入小目标检测界面或者返回主界面。图 5-15（b）为 海滩小目标检测界面，可以通过主界面选择“海滩小目标检测界面”按钮进入或 者接入监控视频界面选择“小目标游客检测”按钮进入，点击“接入视频流”按 钮获得画面信息后可通过“绘制待检测区域按钮”绘制所需检测的危险区域并开 始运行检测模型，当待检测区域出现目标游客后，界面自动给出提示并跳转进入 行为识别界面。图 5-15（c）为溺水行为异常识别界面，可以选择主界面中“溺 水行为异常识别界面”按钮进入或由海滩小目标检测界面自动跳转，系统自动放 大画面信息，并监督行为异常情况，也可点击“行为识别算法模型测试”按钮对 系统所部署的行为识别方法进行性能测试。图 5-15（d）为退出系统展示，点击 主界面中“退出”按钮，经过确认框选择是否退出系统。各个子界面均可通过各 自界面中“返回主界面”按钮回到主界面。 

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.148.jpeg)

- a）接入监控视频界面运行展示                     （b）海滩小目标检测界面运行展示      

![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.149.jpeg)

- c）溺水行为异常识别界面运行展示                    （d）退出系统展示                  

图 5-15 系统各功能实物展示效果示意图 

5. 本章小结<a name="_page62_x87.00_y458.92"></a> 

本章主要介绍了本课题所提海滩监控的小目标检测和行为识别系统研究，基 于多层特征图信息融合的海滩小目标检测方法与融合改进的Alphapose的时空图 卷积网络。首先，本课题从实际场景调研出发，分析当前海滩监控系统在危险区 域游客生命保障时的不足，引出系统需求，同时介绍了系统的开发环境与所用到 的开发工具；其次，分别介绍了系统的硬件框架、软件框架以及系统整体流程， 明确了系统各部分功能需求；接着，通过不同界面功能需求分析并搭建相关 GUI 界面，介绍了各个界面所用到的控件名称及其功能；最后通过系统监控危险区域 小目标游客出没情况并持续放大跟踪游客行为动作，实现了小目标检测与行为识 别功能，并对系统性能进行了评估和分析。通过系统实物运行评估和分析可以看 出，本课题所提海滩监控的小目标检测和行为识别系统能够有效对进入危险区域 的小目标游客开展精确的检测，并基于检测结果对画面进行相应的驱动放大监督 游客行为，有效保障了游客的人身安全。 

<a name="_page63_x87.00_y76.92"></a>总结与展望 

<a name="_page63_x87.00_y126.92"></a>总结 

随着新冠疫情的逐步好转与国家对海洋经济的大力重视，国民走出家门游览 祖国大好河山的强烈愿景，赴海游的热度不断上升，游客在滨海旅游中的安全问 题也显露出来。本课题有效利用海滩监控，结合深度学习技术，重点关注海滩危 险区域游客人生安全问题，设计出海滩监控系统的小目标检测和行为识别系统。 主要研究内容与创新成果如下： 

- 1）针对复杂海滩场景模式下小目标游客检测精度不足的问题，本课题提

出一种基于 YOLO的多层特征图信息融合（FMIF-YOLO）的检测方法，从上下 文信息与强化特征图信息融合的角度提升检出率，解决了当前智能监控检测方案 无法满足此类复杂场景下小目标游客的检测及其后续的准确操作，保障危险区域 中游客的人身安全。首先，透过更全面、有效的 GAM注意力机制思想结合 CSP 结构提出 GCSAM结构，用于增强检测 YOLOv5模型中主干网络端的跨纬度感 受区，聚焦小目标特征学习；其次，在颈部融合端使用 BIFPN结构优化 YOLOv5 网络中 PANet 结构，补全跨层特征信息之间的传递，使得特征图包含更多的上 下文信息；最后，采用幂变换改进 YOLOv5 网络中 CIOU\_Loss 为 Alpha-CIOU\_Loss，有效提升预测框的回归精度。采用网页录屏操作采集监控视 频，间隔 30  s抽取监控视频中 1帧图像形成海滩小目标游客数据集，得到共计 1227 张海滩场景小目标游客图像集用于训练和测试。经过实际的海滩环境进行 改进检测网络的有效性实验，相比于原始 YOLOv5模型，本课题方法在实时性 满足要求的前提下查准率提升 2.00%，查全率提升 5.33%，平均精度均值提升 4.36%，并且获得比其他主流方法更好的主观视觉评价。 

- 2）针对溺水行为中能被检测到的骨架点信息缺失导致行为判断准确率较

低的问题，本课题提出一种融合改进 Alphapose的时空图卷积网络的溺水异常行 为识别算法。在危险区域小目标检测到游客信息后，通过硬件驱动获得目标更为 清晰的画面信息，从充分利用人体骨架点的角度出发。首先，通过 AlphaPose方 法提取人体骨架信息排除环境对行为识别的干扰，引入 YOLOv5作为单人人体

检测器，得到更快更好的人体检测框；其次，将 AlphaPose代替自下而上的方法 HRNet所得的人体关节信息送入 ST-GCN网络训练，避免水下不可观测区域的 错误估计，提高行为识别的精确度；接着，通过自制海滩溺水行为数据视频集弥 补当前针对海面溺水行为数据不足的问题，在国内外公开的视频网站爬取有关溺

水的视频，使用剪辑软件裁剪其中的溺水片段，形成溺水异常行为动作视频集， 得到共计 378段水面溺水行为动作视频集用于训练和测试；最后，结合时间与空 间信息利用所采集到的有限游客关节点信息构建图神经网络，进行实验验证得到 本课题所提方法相比于原始 ST-GCN方法 Top1精确度提升 35.66%，相比于以 Faster-RCNN为检测器的 AlphaPose+ST-GCN方法 Top1精确度提升 6.14%，在复 杂的海面场景下有一个较好的溺水异常行为识别判断结果。 

- 3）基于多层特征图信息融合的海滩小目标检测方法与融合改进的

Alphapose的时空图卷积网络实现一套智能海滩监控系统，此系统包括海滩摄像 头接入、小目标游客检测、危险区域目标游客行为识别、检测效果显示等功能。 首先，本课题从实际场景调研出发，分析海滩监控系统在保障危险区域游客安全 方面的不足，并提出系统需求以及介绍开发环境和工具；其次，说明系统的硬件 框架、软件框架和整体系统流程，明确各部分功能需求；接着，通过不同界面功 能需求分析搭建相关 GUI界面，介绍了各个界面所用到的控件名称及其功能； 最后通过系统监控危险区域小目标游客出没情况并持续放大跟踪游客行为动作， 实现了小目标检测与行为识别功能，并对系统性能进行了评估和分析。实验结果 表明，本课题所提海滩监控的小目标检测和行为识别系统能够有效对进入危险区 域的小目标游客开展精确的检测，并基于检测结果对画面进行相应的驱动放大后 监督游客行为，有效保障了游客的人身安全。 

61 
福州大学硕士学位论文 ![](88305649-d3e7-47c6-bdce-5259c2f4bae9_00000.150.png)

<a name="_page65_x87.00_y76.92"></a>展望 

本课题针对应用于海滩监控系统的小目标检测和行为识别算法及其系统研 究虽取得了一些成果，但无论是理论层次还是技术层次都没有做到十全十美，一 方面是因为个人学识有限，另一方面也是因为工程体量大、应用场景复杂、硬件 设备性能不足等因素所致。未来随着深度学习技术的不断发展，硬件设备的不断 升级，该系统在未来还有很多地方值得改进，尤其是以下几个方面： 

- 1）本课题所设计的溺水行为识别系统仅通过游客上半身甚至更少的肢体

信息进行判断，但依旧仅仅依靠肢体进行粗略判断。由于本课题采用先检测后提 取骨架点的方式获取人体信息，因此可以从目标检测上做文章。目标检测任务是 许多上层任务的基础，其中就包括目标跟踪，因此本系统可以从人物背景、目标 跟踪轨迹以及肢体动作三个方面进行综合研判，有效提升判断的准确性降低一些 由于动作相似引起的误判。 

- 2）本课题所针对的场景为海滩区域中的礁石危险区域，没有关注位于海

中游泳的游客人生安全问题，一方面是由于滨海游泳动作数据集难以收集，另一 方面也是当前系统对游泳与溺水的区分度不够明显，很容易造成误判，导致救援 资源的浪费，如何区分游泳与溺水动作同样值得探索。 

- 3）本课题由于工作量较为庞大，软硬件之间衔接、模型与模型之间的级

联过多，在这些地方都没有做进一步的优化处理，使之达到一个完美的状态。 GUI界面布局简陋，不够美观大气，界面间的跳转方式过于生硬，系统流程还存 在些许不适合实际应用场景的理论设想，同时所有实验都是基于数据集情况下验 证，需要真实场景的验证。 

<a name="_page66_x87.00_y72.92"></a>参考文献 

1. 陆旭<a name="_page66_x87.00_y122.92"></a>, 张弛, 时健, 等. 我国海滩游客安全事故数据库和事故特征分析[J]. 海洋开发与 管理, 2021, 38(6): 3-11. 
1. 自然资源部海洋战略规划与经济司<a name="_page66_x87.00_y159.92"></a>. 2021 年中国海洋经济统计公报[R]. 2022. 
1. 云飞<a name="_page66_x87.00_y179.92"></a>. 5G 落地，要精准发力更要长效机制[J]. 信息化建设, 2019 (8): 1-1. 
1. Paul<a name="_page66_x87.00_y199.92"></a> V, Michael J. Rapid object detection using a boosted cascade of simple features[C]. Proceedings  of  the  2001  IEEE  Computer  Society  Conference  on  Computer  Vision  and Pattern Recognition, Colorado Springs, USA, 2001: 511-518. 
1. Li<a name="_page66_x87.00_y253.92"></a> Q B, Li X, Zhang G J. A hyperspectral small target detection method based on outlier detection[J]. Guangpuxue Yu Guangpu Fenxi, 2009, 28(8): 1832-1836. 
1. Xing<a name="_page66_x87.00_y290.92"></a> M, Su J, Wang G, et al. New parameter estimation and detection algorithm for high speed small target[J]. IEEE Transactions on Aerospace and Electronic Systems, 2011, 47(1): 214-224. 
1. 谷雨<a name="_page66_x87.00_y344.92"></a>, 刘俊, 沈宏海, 等. 基于改进多尺度分形特征的红外图像弱小目标检测[J]. 光学 精密工程, 2020, 28(6):1375-1386. 
1. Krizhevsky<a name="_page66_x87.00_y381.92"></a> A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural  networks[J].  Advances  in  Neural  Information  Processing  Systems,  2012,  25(2): 1097-1105. 
1. 鞠默然<a name="_page66_x87.00_y435.92"></a>, 罗海波, 刘广琦, 等. 采用空间注意力机制的红外弱小目标检测网络[J]. 光学 精密工程, 2021, 29(04): 843-853. 
1. Huang<a name="_page66_x87.00_y472.92"></a>  S  Q,  Liu  Q.  Addressing  scale  imbalance  for  small  object  detection  with  dense detector[J]. Neurocomputing, 2022, 473: 68-78. 
1. 朱威<a name="_page66_x87.00_y509.92"></a>, 王立凯, 靳作宝, 等. 引入注意力机制的轻量级小目标检测网络[J]. 光学精密工 程, 2022, 30(08): 998-1010. 
1. Cubuk<a name="_page66_x87.00_y546.92"></a> E D, Zoph B, Mane D, et al. Autoaugment: Learning augmentation strategies from data[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 2019: 113-123. 
1. Wang<a name="_page66_x87.00_y600.92"></a>  H,  Song  Y,  Huo  L,  et  al.  Multiscale  object  detection  based  on  channel  and  data enhancement at construction sites[J]. Multimedia Systems, 2022: 1-10. 
1. Yin<a name="_page66_x87.00_y637.92"></a> Z, Xu L, Yu H, et al. Data enhancement method for object detection[J]. Journal of Physics: Conference Series. IOP Publishing, 2022, 2209(1): 12-27.  
1. Kisantal<a name="_page66_x87.00_y674.92"></a> M, Wojna Z, Murawski J, et al. Augmentation for small object detection[C]. 9th International Conference on Advances in Computing and Information Technology, Sydney, Australia, 2019: 119-133. 
1. Shrivastava<a name="_page66_x87.00_y728.92"></a> A, Gupta A. Contextual priming and feedback for Faster R-CNN[C]. Proceedings of the European Conference on Computer Vision, Amsterdam, Netherlands, 2016: 330-348. 
17. Cai<a name="_page67_x87.00_y72.92"></a> Z W, Fan Q F, Feris R S, et al. A unified multi-scale deep convolutional neural network for fast object detection[C]. Proceedings of the European Conference on Computer Vision, Amsterdam, Netherlands, 2016: 354-370. 
17. Bell<a name="_page67_x87.00_y126.92"></a> S, Zitnick C L, Bala K, et al. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural network[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 2016: 2874-2883. 
17. Singh<a name="_page67_x87.00_y180.92"></a> B, Davis L S. An analysis of scale invariance in object detection snip[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 2018: 3578-3587. 
17. Zhong<a name="_page67_x87.00_y234.92"></a> Y, Wang J, Peng J, et al. Anchor box optimization for object detection[C]. Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Snowmass, CO, USA, 2020: 1286-1294. 
17. Zhang<a name="_page67_x87.00_y288.92"></a>  S,  Zhu  X,  Lei  Z,  et  al.  Faceboxes:  A  CPU  real-time  face  detector  with  high accuracy[C].  Proceedings  of  the  IEEE  Conference  on  Computer  Vision  and  Pattern Recognition, Denver, CO, USA, 2017: 1-9. 
17. Zhu<a name="_page67_x87.00_y342.92"></a>  C,  Tao  R,  Lu  K,  et  al.  Seeing  small  faces  from  robust  anchor’s  perspective[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 2018: 5127-5136. 
17. Du<a name="_page67_x87.00_y396.92"></a> J, Lu H, Hu M, et al. CNN-based infrared dim small target detection algorithm using target-oriented shallow-deep features and effective small anchor[J]. IET Image Processing, 2021, 15(1): 1-15. 
17. Goodfellow<a name="_page67_x87.00_y450.92"></a> I, Pouget A J, Mirza M, et al. Generative adversarial nets[C]. Proceedings of International  Conference  on  Neural  Information  Processing  Systems,  Montreal,  Canada, 2014: 2672-2680. 
17. Li<a name="_page67_x87.00_y504.92"></a> J A, Liang X D, Wei Y C, et al. Perceptual generative adversarial networks for small object detection[C].  Proceedings  of  the  IEEE  Conference  on  Computer  Vision  and  Pattern Recognition, Honolulu, HI, USA, 2017: 1951-1959. 
17. Bai<a name="_page67_x87.00_y558.92"></a>  Y,  Zhang  Y,  Ding  M,  et  al.  SOD-MTGAN:  Small  object  detection  via  multi-task generative adversarial network[C]. Proceedings of the European Conference on Computer Vision, Munich, Germany, 2018, 11217: 210-226. 
17. Huang<a name="_page67_x87.00_y612.92"></a> W Q, Huang M Z, Zhang Y T. Detection of traffic signs based on combination of GAN  and  faster-rcnn[J].  Journal  of  Physics:  Conference  Series.  IOP  Publishing,  2018, 1069(1): 012159. 
17. Shao<a name="_page67_x87.00_y666.92"></a> L, Gao R, Liu Y, et al. Transform based spatio-temporal descriptors for human action recognition[J]. Neurocomputing, 2011, 74(6): 962-973. 
17. Ji<a name="_page67_x87.00_y703.92"></a> X F, Wu Q Q, Ju Z J, et al. Study of Human Action Recognition Based on Improved Spatio-temporal Features[J]. International Journal of Automation & Computing, 2014, 11(05): 500-509. 
30. Thomas<a name="_page68_x87.00_y72.92"></a> B, Andrés B, Nils P,  et al. High Accuracy Optical Flow Estimation Based on a Theory for Warping[J]. Lecture Notes in Computer Science, 2004, 3024(1): 25-36. 
30. Jiang<a name="_page68_x87.00_y109.92"></a> Y G, Dai Q, Xue X Y, et al. Trajectory-Based Modeling of Human Actions with Motion Reference Points[J]. Lecture Notes in Computer Science. 2012, 7576(1): 425-438. 
30. Yi<a name="_page68_x87.00_y146.92"></a> Y, Lin Y. Human action recognition with salient trajectories[J]. Signal Processing, 2013, 93(11): 2932-2941. 
30. Wang  L,  Qiao  Y,  Tang  X.  Action  recognition  and  detection  by  combining  motion  and appearance features[J]. Thumos14 Action Recognition Challenge, 2014, 1(2): 2. 
30. Liu D, Yan Y, Shyu M L, et al. Spatio-temporal analysis for human action detection and recognition  in  uncontrolled  environments[J].  International  Journal  of  Multimedia  Data Engineering and Management, 2015, 6(1): 1-18. 
30. Zhang<a name="_page68_x87.00_y274.92"></a>  X,  Li  C,  Sun  L,  et  al.  Behavior  recognition  method  based  on  improved  3D convolutional  neural  network[J].  Jisuanji  Jicheng  Zhizao  Xitong/Computer  Integrated Manufacturing Systems, 2019, 25(8): 2000-2006. 
30. Zhang<a name="_page68_x87.00_y328.92"></a>  K,  Li  D,  Huang  J,  et  al.  Automated  video  behavior  recognition  of  pigs  using two-stream convolutional networks[J]. Sensors, 2020, 20(4): 1085. 
30. Hsueh<a name="_page68_x87.00_y365.92"></a> Y L, Lie W N, Guo G Y. Human behavior recognition from multiview videos[J]. Information Sciences, 2020, 517: 275-296. 
30. Guan<a name="_page68_x87.00_y402.92"></a>  Y,  Hu  W,  Hu  X.  Abnormal  behavior  recognition  using  3D-CNN  combined  with LSTM[J]. Multimedia Tools and Applications, 2021, 80(12): 18787-18801. 
30. Byeon<a name="_page68_x87.00_y439.92"></a> Y H, Kim D, LEE J, et al. Ensemble Three-Stream RGB-S Deep Neural Network for Human Behavior Recognition Under Intelligent Home Service Robot Environments[J]. IEEE Access, 2021, 9: 73240-73250. 
30. Rezaei<a name="_page68_x87.00_y493.92"></a> F, Yazdi M. Real-time crowd behavior recognition in surveillance videos based on deep learning methods[J]. Journal of Real-Time Image Processing, 2021, 18(5): 1669-1679. 
30. Johansson<a name="_page68_x87.00_y530.92"></a>  G.  Visual  perception  of  biological  motion  and  a  model  for  its  analysis[J]. Perception & Psychophysics, 1973, 14(2): 201-211. 
30. Hubel<a name="_page68_x87.00_y567.92"></a> D H, Wiesel T N. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex[J]. The Journal of Physiology, 1962, 160(1): 106. 
30. Simonyan<a name="_page68_x87.00_y604.92"></a>  K,  Zisserman  A.  Very  deep  convolutional  networks  for  large-scale  image recognition[J]. ArXiv:1409.1556, 2014. 
30. He<a name="_page68_x87.00_y641.92"></a> K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, USA, 2016: 770-778. 
30. Redmon<a name="_page68_x87.00_y695.92"></a>  J,  Divvala  S,  Girshick  R,  et  al.  You  only  look  once:  Unified,  real-time  object detection[C].  Proceedings  of  the  IEEE  Conference  on  Computer  Vision  and  Pattern Recognition, Las Vegas, USA, 2016: 779-788. 
46. Redmon  J,  Farhadi  A.  YOLO9000:  better,  faster,  stronger[C].  Proceedings  of  the  IEEE Conference on Computer Vision and Pattern Recognition, Hawaii, USA, 2017: 7263-7271. 
46. Redmon J, Farhadi A. Yolov3: An incremental improvement[J]. ArXiv:1804.02767, 2018. 
46. Bochkovskiy A, Wang C Y, Liao H Y M. Yolov4: Optimal speed and accuracy of object detection[J]. ArXiv:2004.10934, 2020. 
46. Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Columbus, Ohio, USA, 2014: 580-587. 
46. Girshick R. Fast r-cnn[C]. Proceedings of the IEEE International Conference on Computer Vision, Santiago, Chile, 2015: 1440-1448. 
46. Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[J]. Advances in Neural Information Processing Systems, 2015, 28. 
46. Yu<a name="_page69_x87.00_y294.92"></a>  F,  Koltun  V.  Multi-scale  context  aggregation  by  dilated  convolutions[J]. ArXiv:1511.07122, 2015. 
46. Dai<a name="_page69_x87.00_y331.92"></a> J, Qi H, Xiong Y, et al. Deformable convolutional networks[C]. Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 2017: 764-773. 
46. Chollet<a name="_page69_x87.00_y368.92"></a> F. Xception: Deep learning with Depthwise separable convolutions[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Hawaii, USA, 2017: 1251-1258. 
46. Maas<a name="_page69_x87.00_y422.92"></a> A L, Hannun A Y, Ng A Y. Rectifier nonlinearities improve neural network acoustic models[J]. Proc. Icml, 2013, 30(1): 3-3. 
46. Bruna<a name="_page69_x87.00_y459.92"></a> J, Zaremba W, Szlam A. Spectral networks and deep locally connected networks on graph[J]. ArXiv:1312.6203, 2013. 
46. T.n.kipf,<a name="_page69_x87.00_y496.92"></a>  M.welling.  Semi-supervised  classification  with  graph  convolutional  networks[J]. ArXiv:1609.02907, 2016. 
46. Lim J, Astrid M, Yoon H, et al. Small object detection using context and attention[C]. 2021 International Conference on Artificial Intelligence in Information and Communication, Jeju Island, Korea, 2021: 181-186.  
46. Leng  J  X,  Ren  Y  H,  Jiang  W,  et  al.  Realize  Your  Surroundings:  Exploiting  Context Information for Small Object Detection[J]. Neurocomputing, 2021, 433(8): 287-299. 
46. Chen J Y, Liu S P, Zhao L, et al. Small object detection combining attention mechanism and a novel FPN[J]. Journal of Intelligent & Fuzzy Systems, 2021: 1-13. 
46. Liu<a name="_page69_x87.00_y661.92"></a> Y C, Shao Z G, Hoffmann N. Global attention mechanism: retain information to enhance channel-spatial interactions[J]. ArXiv:2112.05561, 2021. 
46. Woo<a name="_page69_x87.00_y698.92"></a> S, Park J, Lee J, et al. CBAM: Convolutional block attention module[C]. Proceedings of the European Conference on Computer Vision, Munich, Germany, 2018: 3-19. 
63. Hu<a name="_page70_x87.00_y72.92"></a>  J,  Shen  L,  Sun  G.  Squeeze-and-excitation  networks[C].  Proceedings  of  the  IEEE Conference  on  Computer  Vision  and  Pattern  Recognition,  Salt  Lake  City,  USA,  2018: 7132-7141. 
63. Liu<a name="_page70_x87.00_y126.92"></a>  S,  Qi  L,  Qin  H  F,  et  al.  Path  aggregation  network  for  instance  segmentation[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, USA, 2018: 8759-8768. 
63. Tan<a name="_page70_x87.00_y180.92"></a>  M  X,  Pang  R  M,  Le  Q.  EfficientDet:  Scalable  and  efficient  object  detection[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and  Pattern Recognition, Seattle, USA, 2020: 10781-10790. 
63. He<a name="_page70_x87.00_y234.92"></a> J B, Erfani S, Ma X J, et al. Alpha-IOU: a family of power intersection over union losses for bounding box regression[J]. ArXiv:2110.13675, 2022. 
63. Liu<a name="_page70_x87.00_y271.92"></a> W, Anguelov D, Erhan D, et al. SSD: Single shot multibox detector[J]. Computer Vision 
    1. ECCV 2016, 2016, 9905: 21-37. 
63. Zhu<a name="_page70_x87.00_y308.92"></a>  P,  Wen  L,  Du  D,  et  al.  Detection  and  tracking  meet  drones  challenge[J].  IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021, 44(11): 7380-7399. 
63. Lin<a name="_page70_x87.00_y345.92"></a> T Y, Maire M, BELONGIE S, et al. Microsoft coco: Common objects in context[C]. European Conference on Computer Vision. Springer, Cham, 2014: 740-755. 
63. Cao<a name="_page70_x87.00_y382.92"></a> Z, Simon T, Wei S E, et al. Realtime multi-person 2d pose estimation using part affinity fields[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Hawaii, USA, 2017: 7291-7299. 
63. Fang<a name="_page70_x87.00_y436.92"></a>  H  S,  Xie  S,  Tai  Y  W,  et  al.  RMPE:  Regional  multi-person  pose  estimation[C]. Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 2017: 2334-2343. 
63. Newell<a name="_page70_x87.00_y490.92"></a>  A,  Yang  K,  Deng  J.  Stacked  hourglass  networks  for  human  pose  estimation[C]. European Conference on Computer Vision. Springer, Cham, 2016: 483-499. 
63. Yan<a name="_page70_x87.00_y527.92"></a> S, Xiong Y, Lin D. Spatial temporal graph convolutional networks for skeleton-based action  recognition[C].  Thirty-Second  AAAI  Conference  on  Artificial  Intelligence,  Hefei, China, 2018. 
63. Sun<a name="_page70_x87.00_y581.92"></a> K, Xiao B, Liu D, et al. Deep high-resolution representation learning for human pose estimation[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, California, USA, 2019: 5693-5703. 
63. Bai<a name="_page70_x87.00_y635.92"></a> S, Kolter J Z, Koltun V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling[J]. ArXiv:1803.01271, 2018. 
63. Soomro<a name="_page70_x87.00_y672.92"></a> K, Zamir A R, Shah M. UCF101: A dataset of 101 human actions classes from videos in the wild[J]. ArXiv:1212.0402, 2012. 

<a name="_page71_x87.00_y72.92"></a>在学期间的研究成果及发表的学术论文 

在读期间已发表和录用的论文：** 

第一作者（2篇）： 

- 1】多层特征图信息融合的海滩小目标检测[J]. 光电子技术. 
- 2】Research on Small Target Detection and Behavior Recognition Algorithm for Beach Monitoring System[C]. SID Symposium Digest of Technical Papers, Fuzhou, China, 2022, 53(S1): 827-827. 

第三作者（1篇）： 

- 1】图像边缘权重优化的最小生成树分割提取[J]. 电子与信息学报,  2022,  44: 1-11.  

参与的科研项目及成果：** 

- 1】国家重点研发计划课题(NO:  2021YFB3600603): 彩色电子纸显示器集成与 驱动 
- 2】福建省自然科学基金资助项目(NO: 2020J01468): 电润湿电子纸动力学非稳 态建模与画质优化的研究 
- 3】福建省教育厅中青年教师教育科研项目(NO: JAT210030): 基于机器视觉的 多目标垃圾分类系统研究 
- 4】一种海滩监控溺水防护系统，申请号：CN202210681766.5 
- 5】一种基于机器视觉的智能海滩安全防护方法及系统，申请号： CN202111533554.4 
- 6】一种液晶显示三畴配向层的光配向光路系统，申请号：CN202110108677.7 
67 
